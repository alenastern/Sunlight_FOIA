{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# system tools\n",
    "import warnings\n",
    "import json\n",
    "import sys\n",
    "import string\n",
    "import ast\n",
    "\n",
    "# data cleaning + analysis tools\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#nltk tools\n",
    "import spacy\n",
    "import lda #Latent Dirichlet Allocation (create topics)\n",
    "import gensim\n",
    "from gensim import corpora, models #for constructing document term matrix\n",
    "#from stop_words import get_stop_words\n",
    "from gensim.models import Phrases\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#set notebook preferences\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import JSON file with city metadata \n",
    "\n",
    "This including which cities have published the raw Public Record Requests (PRRs) they receive for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = '../data/cities.json'\n",
    "\n",
    "with open(json_file, 'r') as f:\n",
    "    md = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create dataframe of PRR data for all relevant cities\n",
    "\n",
    "This dataframe includes PPR data from the **33 cities** in our sample (52 total cities) that had sufficient raw data with the public record request for analysis. Our sample represents cities that host an online PRR portal for submitting requests.  These data were obtained through a variety of methods including:\n",
    "\n",
    "1. exporting the full archive of PRRs hosted on the online portal as a csv file  \n",
    "2. scraping the full history of PRR data from portals which publish previous requests, but do not offer a download option (see [scraping notebook](https://github.com/sunlightpolicy/Sunlight_FOIA/blob/master/src/data/NR_Scrape.ipynb))\n",
    "3. downloading public records request data that has been published on city’s open data portal  \n",
    "4. submitting a public record request to obtain the archive of PRR data \n",
    "\n",
    "It is worth noting that *specificities of the different city portals influence the substance of the public record requests received*. For example, the city of Clearwater, FL has separate request forms for police records and public records, prompting citizens who submit police record requests to provide the specific case number. In addition, while most of the data released by cities is the raw request submitted by citizens, in a few cases the city released a summary of the submitted request prepared by city staff. For example, the Oklahoma City clerk's office released the summary of the request and the department the request was routed to for response. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arlington city\n",
      "Bainbridge Island city\n",
      "Boulder County\n",
      "Cathedral City city\n",
      "Clearwater city\n",
      "Dayton city\n",
      "Denton city\n",
      "Everett city\n",
      "Fort Collins city\n",
      "Greensboro city\n",
      "Hayward city\n",
      "Kirkland city\n",
      "Las Cruces city\n",
      "Lynnwood city\n",
      "Mercer Island city\n",
      "Miami city\n",
      "Middleborough town\n",
      "New Orleans city\n",
      "Oakland city\n",
      "Oklahoma City city\n",
      "Olympia city\n",
      "Palo Alto city\n",
      "Peoria city\n",
      "Pullman city\n",
      "Rancho Cucamonga city\n",
      "Redmond city\n",
      "Renton city\n",
      "Sacramento city\n",
      "San Francisco city\n",
      "Tukwila city\n",
      "Vallejo city\n",
      "West Sacramento city\n",
      "Winchester city\n"
     ]
    }
   ],
   "source": [
    "data_raw = pd.DataFrame(columns = ['city', 'month_year', 'Summary'])\n",
    "city_list = []\n",
    "for key, value in md.items():\n",
    "    city = value['name']\n",
    "    filepath = '/Users/alenastern/Google Drive File Stream/My Drive/Alena_Project/PR_Data/{}.csv'.format(city)\n",
    "    # tag in metadata for whether city publishes request content\n",
    "    if value[\"desc\"] == \"Y\":\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding='mac_roman')\n",
    "            except:\n",
    "                continue\n",
    "        print(key)\n",
    "        name = key.split(' ')\n",
    "        city_list.extend([x for x in name[:-1]])\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df['Create Date'] = pd.to_datetime(df['Create Date'])\n",
    "    except:\n",
    "        df['New'] = pd.to_datetime(df['Create Date'].apply(lambda x: re.findall('^\\S*', x)[0]))\n",
    "        df.drop(columns=['Create Date'], inplace = True)\n",
    "        df.rename(index=str, columns={\"New\": \"Create Date\"}, inplace = True)\n",
    "\n",
    "    df['month_year'] = df['Create Date'].dt.to_period('M')\n",
    "    \n",
    "    mc = df[['month_year', 'Summary']]\n",
    "    mc['city'] = city\n",
    "    \n",
    "    data_raw = pd.concat([data_raw, mc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our raw dataset includes 110,063 PRRs from 33 different cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110063, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.to_csv('data_raw.csv')\n",
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_raw.city.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sequential numeric index for data\n",
    "\n",
    "data_raw.index = pd.RangeIndex(len(data_raw.index))\n",
    "data_raw.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe for cleaning by removing null summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop observations that are null for the raw PRR content field ('Summary')\n",
    "data = data_raw.dropna(subset=['Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the raw data below. As we can see, the text in the Summary field is very messy and will require a lot of cleaning to prepare the data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Summary</th>\n",
       "      <th>city</th>\n",
       "      <th>month_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>We are working with an engineering firm on an upcoming project.  They have asked us to gather maps for this project.  Would you be able to assist me in gathering maps/records (as builds) for any underground water facilities you may have?  Something just showing the route of the water lines would do.\\n\\n207th ST NE to 92nd Ave NE, Arlington, Cascade Surveying &amp; Engineering \\n\\nI have attached the scope for your convenience.  Please let me know if you have questions.</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Need copies of contracts and all related documents pertaining to Topcub Aircraft property located at 17922 59th DR NE Arlington WA 98223 between Arlington Airport, Topcub Aircraft, City of Arlington, HCI Steel Buildings and PUD.</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Copies of Building Permits of $5,000 valuation and up ($20,000 min for Re-Roofs), ($50,000 min. for Cell Tower upgrades), (Electrical, Mechanical &amp; Plumbing at $100,000 min.) and (Solar Panels, Swimming Pools &amp; Foundations at any valuation)</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>police report filed to an officer against Wayne Parris (DOB 08-03-1957) from Brittany J. Parris. The paperwork I have has a case number D18-39 it is also stamped at the bottom with 18-1294, Iím not sure which number you will need. If there is any other information needed please let me know.</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Email Communications between Stephanie Shook, Dave Kraski, Bruce Stedman and Chad Schmidt in regards to Fire Protection District 21 billing and passage of contract for ALS Services. \\n\\nAlso any copies of Agenda Bills, D21 Contract and materials presented for review in Nov/Dec time frame in regards to the contract.</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Summary       city month_year\n",
       "0      0  We are working with an engineering firm on an upcoming project.  They have asked us to gather maps for this project.  Would you be able to assist me in gathering maps/records (as builds) for any underground water facilities you may have?  Something just showing the route of the water lines would do.\\n\\n207th ST NE to 92nd Ave NE, Arlington, Cascade Surveying & Engineering \\n\\nI have attached the scope for your convenience.  Please let me know if you have questions.  Arlington    2018-06\n",
       "1      1                                                                                                                                                                                                                                                   Need copies of contracts and all related documents pertaining to Topcub Aircraft property located at 17922 59th DR NE Arlington WA 98223 between Arlington Airport, Topcub Aircraft, City of Arlington, HCI Steel Buildings and PUD.  Arlington    2018-06\n",
       "2      2                                                                                                                                                                                                                                       Copies of Building Permits of $5,000 valuation and up ($20,000 min for Re-Roofs), ($50,000 min. for Cell Tower upgrades), (Electrical, Mechanical & Plumbing at $100,000 min.) and (Solar Panels, Swimming Pools & Foundations at any valuation)  Arlington    2018-06\n",
       "3      3                                                                                                                                                                                    police report filed to an officer against Wayne Parris (DOB 08-03-1957) from Brittany J. Parris. The paperwork I have has a case number D18-39 it is also stamped at the bottom with 18-1294, Iím not sure which number you will need. If there is any other information needed please let me know.  Arlington    2018-06\n",
       "4      4                                                                                                                                                           Email Communications between Stephanie Shook, Dave Kraski, Bruce Stedman and Chad Schmidt in regards to Fire Protection District 21 billing and passage of contract for ALS Services. \\n\\nAlso any copies of Agenda Bills, D21 Contract and materials presented for review in Nov/Dec time frame in regards to the contract.  Arlington    2018-06"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert nltk part of speech tags to wordnet tags (we use this to stem the words in data cleaning below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A key challenge is removing proper names from the raw data. We initially tried using the NLTK and SpaCy proper noun tags, but ultimately found the best performance from using a dictionary of common first and last names. To create this dictionary, we first write a function to turn separate files of the 1000 most popular baby names by year provided by the [Social Security Administration](https://www.ssa.gov/OACT/babynames/) into a single set of unique first names across years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_list(st, end):\n",
    "    names = set()\n",
    "    for yr in range(st, end+1):\n",
    "        fp = '../data/names/yob{}.txt'.format(yr)\n",
    "        df = pd.read_table(fp, sep = ',', names = ['name', 'sex', 'count'])\n",
    "        names |= set(df['name'])\n",
    "    \n",
    "    return list(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create list of unique first names that were on the 1000 most popular names each year between 1950-2017\n",
    "\n",
    "names = name_list(1950,2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create list on common surnames in the United States. Data on surnames is from the U.S. Census Bureau, compiled by FiveThirtyEight and accessed via [data.world](https://data.world/fivethirtyeight/most-common-name/workspace/file?filename=README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_names = pd.read_csv('../data/names/surnames.csv')\n",
    "last_names.name = last_names.name.str.title()\n",
    "ln = list(last_names['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine first names and surnames and create dictionary \n",
    "\n",
    "all_names = names + ln\n",
    "all_names_dict = {key: 1 for key in all_names if key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean PRR data to prepare for LDA analysis\n",
    "\n",
    "Prior to analysis, we clean our unstructured text data to improve the outcome of our LDA analysis results. Our goals are as follows:\n",
    "\n",
    "1. Remove \"noise\" - words that do not provide information on the subject of a PRR (eg. stop words like \"the\", proper nouns like people's names or city names, punctuation and digits, and general words/phrase common to PRRs like \"good morning\" or \"record\"\n",
    "2. Stem words so like words are treated as the same (eg. \"photo\" and \"photos\" should be regarded as the same word, as should \"assault\" and \"assaulted\"\n",
    "3. Account for meaningful phrases where the combination of words has particular meaning (to avoid excessive computation time, we only consider two-word phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a copy of the original request column to preserve\n",
    "data['summary_raw'] = data['Summary']\n",
    "\n",
    "# remove common public record request phrases - we remove as phrases because we care about specific combination/order \n",
    "# of words (we want to remove \"open record request\" not all instances of word \"open\")\n",
    "phrase_list = ['public record request', 'open record request', 'public records request', 'open records request', \n",
    "               'foia request', 'see attached', 'see attachment', 'to whom it may concern', 'public records act',\n",
    "              'electronic copy', 'electronic copies', 'freedom of information act', 'good afternoon', 'good morning',\n",
    "              'good day']\n",
    "                         \n",
    "for phrase in phrase_list:\n",
    "    s = re.compile(re.escape(phrase), re.IGNORECASE)\n",
    "    data.Summary = data['Summary'].apply(lambda x: s.sub('', x))\n",
    "    \n",
    "    \n",
    "# Replace common acronyms in Summary\n",
    "data.Summary = data.Summary.str.replace('NOPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('OPD' , 'police department')\n",
    "data.Summary = data.Summary.str.replace('SFPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('CPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('APD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('GPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('KPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('TPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('DPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('EPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('HPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('LPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('MDPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('PPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('SPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('VPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('CCPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('FCPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('TPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('LCPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('OKCPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('PAPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('RCPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('WSPD', 'police department')\n",
    "\n",
    "# PDRD = portable digital recording device (body cam) worn by police\n",
    "data.Summary = data.Summary.str.replace('PDRD', 'police body camera')\n",
    "data.Summary = data.Summary.str.replace('CPS', 'child protective services')\n",
    "\n",
    "#https://www.sfdph.org/dph/EH/HMUPA/HMUPAFormsMenu.asp - hazardous materials\n",
    "#https://www.waterboards.ca.gov/ust/contacts/docs/lop_guide.pdf - water resources local oversight program\n",
    "data.Summary = data.Summary.str.replace('LOP', 'water')\n",
    "data.Summary = data.Summary.str.replace('HMUPA', 'hazardous materials')\n",
    "\n",
    "# Replace key numbers with strings\n",
    "data.Summary = data.Summary.str.replace(' 911 ', ' nineoneone ')\n",
    "data.Summary = data.Summary.str.replace(' 311 ', ' threeoneone ')\n",
    "data.Summary = data.Summary.str.replace(' 9-11 ', ' nineoneone ')\n",
    "data.Summary = data.Summary.str.replace(' 3-11 ', ' threeoneone ')\n",
    "\n",
    "# Remove digits\n",
    "dig_translator = str.maketrans('','', string.digits)\n",
    "data.Summary = data.Summary.str.translate(dig_translator)\n",
    "\n",
    "# because \"will\" is in the NLTK list of stopwords below, we treat 'final will' separately                         \n",
    "c = re.compile(re.escape('final will'), re.IGNORECASE)\n",
    "data.Summary = data['Summary'].apply(lambda x: s.sub('final final_will', x))\n",
    "\n",
    "# replace hyphen and slash with space to treat hyphate words as two separate words\n",
    "hyphen_translator = str.maketrans('-/','  ')\n",
    "data.Summary = data.Summary.str.translate(hyphen_translator)\n",
    "\n",
    "# remove all punctuation\n",
    "translator = str.maketrans('','', string.punctuation)\n",
    "data.Summary = data.Summary.str.translate(translator)\n",
    "\n",
    "# split text into list of words by space \n",
    "data['token'] = data['Summary'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# remove proper first and last names in our dictionary + convert all words to lower case\n",
    "data['token'] = data['token'].apply(lambda x: [i.lower() for i in x if i not in all_names_dict])\n",
    "\n",
    "#remove empty strings, stopwords and stem\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lmtzr = WordNetLemmatizer()\n",
    "data['lemma'] = data['token'].apply(lambda x: nltk.pos_tag(x))\n",
    "data['mash'] = data['lemma'].apply(lambda x: [lmtzr.lemmatize(i[0], get_wordnet_pos(i[1])) for i in x if len(i[0]) > 0 and i[0] not in stop_words])\n",
    "\n",
    "# Remove whitespace\n",
    "wsp_translator = str.maketrans('','', string.whitespace)\n",
    "data['mash'] = data['mash'].apply(lambda x: [i.translate(wsp_translator) for i in x])\n",
    "\n",
    "# Remove empty lists\n",
    "data['mash_len'] = data['mash'].apply(lambda x: len(x))\n",
    "data = data[data['mash_len'] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove number suffixes\n",
    "suffix_list = ['th', 'nd', 'st', 'rd', 'blvd', 'pkwy']\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in suffix_list])\n",
    "\n",
    "# remove city and state abbreviations\n",
    "abbv_list = ['wa', 'nc', 'co', 'ca', 'oh', 'tx', 'nm', 'fl', 'ma', 'la', 'ok', 'az', 'ri', 'va', \n",
    "             'francisco', 'sf', 'okc', 'lv', 'nola', 'slc', 'cw']\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in abbv_list])\n",
    "\n",
    "# remove spelled numbers\n",
    "num_list = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten']\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in  num_list])\n",
    "\n",
    "# replace 'inc' with 'incident\n",
    "data['mash'] = data['mash'].apply(lambda x: ['incident' if i=='inc' else i for i in x])\n",
    "\n",
    "# replace 'pd' with 'police department\n",
    "data['mash'] = data['mash'].apply(lambda x: ['police department' if i=='pd' else i for i in x])\n",
    "\n",
    "# remove noise words\n",
    "noise = ['dr', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sept', 'sep', 'oct', 'nov', 'dec', \n",
    "        'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', \n",
    "        'december', 'ne', 'nw', 'se', 'sw', 'ct', 'dr', 'way', 'dv', 'ave', 'aka', 'get', 'look', 'im', 'want', \n",
    "        'find', 'could', 'go', 'take', 'e', 'n', 's', 'w', '“', '’', '”', '•', 'northeast', 'northwest', 'southeast', \n",
    "        'southwest', 'north', 'south', 'east', 'west', 'orleans', '–', 'a', 'b', 'c', 'd', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "        'l', 'm', 'o', 'p', 'q', 'r', 't', 'u', 'v', 'x', 'y', 'z', 'am', 'pm', 'hr', 'mr', 'ms', 'mrs', 'johnson', \n",
    "        'jr', 'kent', 'christopher', 'miller', 'joe', 'willows', 'david', 'michael', 'john', 'red', 'robert',\n",
    "        'ask', 'able', 'let', 'question', 'also', 'snohomish', '¬ß', 'per', 'available', 'test', '√Ø', 'andor', '·', 'etc',\n",
    "        'ï', 'ce', 'eg', 'sammamish']\n",
    "\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in noise])\n",
    "\n",
    "# remove cities\n",
    "city_list = ['arlington', 'asheville', 'bainbridge', 'island', 'boulder', 'cathedral' ,'clearwater', 'dayton', \n",
    "            'denton', 'everett', 'fort', 'collins', 'greensboro', 'hayward', 'kirkland', 'las', 'cruces', 'lynnwood',\n",
    "            'mercer', 'miami', 'middleborough', 'new', 'orleans', 'oakland', 'oklahoma', 'olympia', 'palo', 'alto', \n",
    "            'peoria', 'pullman', 'rancho', 'cucamonga', 'redmond', 'renton', 'sacramento', 'san', 'francisco', \n",
    "            'tukwila', 'vallejo', 'west', 'sacramento', 'winchester']\n",
    "\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in city_list])\n",
    "\n",
    "# Remove all state names\n",
    "state_list = ['washington', 'carolina', 'colorado', 'california',\n",
    "             'ohio', 'texas', 'florida', 'new', 'mexico','massachusetts',\n",
    "             'louisiana', 'oklahoma', 'arizona', 'rhode', 'virginia']\n",
    "\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in state_list])\n",
    "\n",
    "# Create two-word phrases (bigrams)\n",
    "data['bigrams'] = data['mash'].apply(lambda x: [\"_\".join(w) for w in ngrams(x, 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify and remove noise words that are commonly used in PRRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('report', 43899),\n",
       " ('request', 31182),\n",
       " ('record', 23179),\n",
       " ('please', 21970),\n",
       " ('copy', 20125),\n",
       " ('property', 13058),\n",
       " ('information', 12020),\n",
       " ('number', 11451),\n",
       " ('provide', 10819),\n",
       " ('would', 10607),\n",
       " ('include', 10494),\n",
       " ('incident', 10264),\n",
       " ('document', 9837),\n",
       " ('police', 9587),\n",
       " ('permit', 9376),\n",
       " ('accident', 9038),\n",
       " ('thank', 8925),\n",
       " ('address', 8436),\n",
       " ('department', 8347),\n",
       " ('like', 8025),\n",
       " ('location', 7985),\n",
       " ('regard', 7839),\n",
       " ('date', 7570),\n",
       " ('email', 7463),\n",
       " ('relate', 6769),\n",
       " ('reference', 6760),\n",
       " ('building', 6719),\n",
       " ('insurance', 6603),\n",
       " ('call', 6573),\n",
       " ('case', 6549),\n",
       " ('following', 6439),\n",
       " ('video', 6264),\n",
       " ('type', 5770),\n",
       " ('violation', 5628),\n",
       " ('file', 5622),\n",
       " ('plan', 5469),\n",
       " ('auto', 5393),\n",
       " ('need', 5212),\n",
       " ('dob', 4978),\n",
       " ('name', 4857),\n",
       " ('insure', 4774),\n",
       " ('public', 4498),\n",
       " ('transaction', 4476),\n",
       " ('city', 4416),\n",
       " ('make', 4366),\n",
       " ('occurrence', 4360),\n",
       " ('project', 4336),\n",
       " ('list', 4318),\n",
       " ('involve', 4190),\n",
       " ('time', 4119)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = [y for x in list(data['mash']) for y in x]\n",
    "counts = Counter(word_list)\n",
    "Counter(word_list).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that general words like \"report\", \"please\", and \"copy\" ae very common, yet do not provide information on the content of the request. We remove these common and general words, keeping common words that provide information like \"property\" and \"police\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_list = ['report', 'request', 'record', 'city', 'please', 'copy', 'date', 'information', 'would', 'regard', 'public',\n",
    "              'include', 'document', 'provide', 'like', 'thank', 'need', 'know', 'thanks', 'pursuant', 'dear', 'file',\n",
    "              'relate', 'from', 'either', 'hello', 'hi', 'foia', 'requestors', 'requestor', 'receive', 'available', \n",
    "               'make', 'attach', 'pertain', 'might', 'see', 'near']\n",
    "\n",
    "# remove general words that are common to public record requests\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in common_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify meaningful phrases by looking at the list of two-word sequences (bigrams) that are frequently used in public record requests. The meaningful phrases that we identify will be added to the list of words to consider in analysis for the PRRs in which they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('would_like', 7167),\n",
       " ('please_provide', 6801),\n",
       " ('accident_report', 5311),\n",
       " ('police_report', 4861),\n",
       " ('report_number', 4851),\n",
       " ('report_type', 4341),\n",
       " ('transaction_reference', 4315),\n",
       " ('auto_accident', 4299),\n",
       " ('occurrence_location', 4227),\n",
       " ('reference_insurance', 4207),\n",
       " ('number_occurrence', 4036),\n",
       " ('type_auto', 3557),\n",
       " ('request_copy', 3259),\n",
       " ('incident_report', 2889),\n",
       " ('offense_report', 2493),\n",
       " ('like_request', 2180),\n",
       " ('code_violation', 2174),\n",
       " ('provide_copy', 2092),\n",
       " ('police_department', 1894),\n",
       " ('include_limited', 1732),\n",
       " ('public_record', 1705),\n",
       " ('property_locate', 1698),\n",
       " ('certificate_occupancy', 1693),\n",
       " ('copy_police', 1621),\n",
       " ('hazardous_material', 1609),\n",
       " ('request_record', 1563),\n",
       " ('building_permit', 1485),\n",
       " ('storage_tank', 1410),\n",
       " ('please_know', 1288),\n",
       " ('request_please', 1274),\n",
       " ('please_advise', 1223),\n",
       " ('please_send', 1198),\n",
       " ('document_relate', 1183),\n",
       " ('please_contact', 1164),\n",
       " ('law_enforcement', 1147),\n",
       " ('copy_report', 1102),\n",
       " ('collision_report', 1090),\n",
       " ('case_number', 1086),\n",
       " ('request_information', 1068),\n",
       " ('record_relate', 1045),\n",
       " ('please_include', 1028),\n",
       " ('like_copy', 962),\n",
       " ('state_farm', 951),\n",
       " ('information_request', 940),\n",
       " ('nineoneone_call', 939),\n",
       " ('insurance_state', 935),\n",
       " ('property_address', 934),\n",
       " ('auto_theft', 900),\n",
       " ('ftp_report', 889),\n",
       " ('request_police', 868),\n",
       " ('location_insure', 867),\n",
       " ('tax_sale', 861),\n",
       " ('request_following', 841),\n",
       " ('site_assessment', 831),\n",
       " ('environmental_site', 824),\n",
       " ('site_plan', 823),\n",
       " ('farm_claim', 820),\n",
       " ('claim_compass', 820),\n",
       " ('duo_packet', 809),\n",
       " ('compass_report', 808),\n",
       " ('report_incident', 788),\n",
       " ('via_email', 780),\n",
       " ('report_photo', 779),\n",
       " ('packet_offense', 762),\n",
       " ('record_request', 755),\n",
       " ('record_regard', 743),\n",
       " ('record_pertain', 739),\n",
       " ('witness_statement', 731),\n",
       " ('king_insure', 716),\n",
       " ('fire_code', 715),\n",
       " ('theft_report', 708),\n",
       " ('duo_video', 701),\n",
       " ('last_year', 693),\n",
       " ('information_regard', 687),\n",
       " ('record_check', 685),\n",
       " ('report_involve', 683),\n",
       " ('last_month', 677),\n",
       " ('copy_case', 669),\n",
       " ('phase_environmental', 668),\n",
       " ('report_date', 668),\n",
       " ('underground_storage', 665),\n",
       " ('within_last', 663),\n",
       " ('video_audio', 643),\n",
       " ('following_property', 634),\n",
       " ('need_copy', 629),\n",
       " ('insurance_report', 620),\n",
       " ('following_information', 615),\n",
       " ('cad_note', 607),\n",
       " ('record_please', 605),\n",
       " ('copy_video', 600),\n",
       " ('following_record', 594),\n",
       " ('copy_following', 593),\n",
       " ('crash_report', 582),\n",
       " ('thurston_insure', 581),\n",
       " ('build_permit', 574),\n",
       " ('copy_record', 572),\n",
       " ('video_duo', 571),\n",
       " ('please_email', 570),\n",
       " ('feel_free', 566),\n",
       " ('permit_issue', 554)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_list = [y for x in list(data['bigrams']) for y in x]\n",
    "counts = Counter(bigram_list)\n",
    "Counter(bigram_list).most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some of the common bigrams do not provide insight (such as \"would_like\" and \"please_provide\") others provide key meaning that is not captured by the individual words such as \"police_report\", \"auto_accident\", and \"building_permit\". We add the meaningful bigrams in the list below to the final set of words for the PRRs in which they occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "common_bigrams = ['police_report', 'insurance_company', 'location_loss', 'date_occurrence', 'reportcase_number',\n",
    "                  'insure_driver', 'auto_accident', 'occurrence_location', 'transactionreference_insurance', 'number_date', 'type_auto',\n",
    "                  'accident_reportcase', 'code_violation', 'copy_police', 'incident_report', 'police_department', 'certificate_occupancy',\n",
    "                  'accident_report', 'property_locate', 'storage_tank','driver_note', 'building_permit', 'driver_driver','case_number', \n",
    "                  'hazardous_material', 'collision_report', 'state_farm', 'site_plan', 'fire_department', 'ftp_report', 'auto_theft',\n",
    "                  'fire_code', 'request_police', 'farm_claim', 'claim_compass', 'site_assessment', 'compass_report', 'environmental_site', \n",
    "                  'tax_sale', 'loss_cross','city_council', 'code_enforcement', 'subject_property', 'report_case', 'phase_environmental', \n",
    "                  'report_incident', 'date_loss', 'police_case', 'witness_statement', 'driving_record', 'break_in', 'birth_certificate', \n",
    "                  'death_certificate', 'background_check', 'public_works', 'lease_agreement', 'medical_record', 'billing_record', \n",
    "                  'record_check', 'records_check', 'marriage_certificate', 'marriage_record', 'park_ticket', 'miss_person',\n",
    "                 'marriage_license', 'reckless_driving', 'arrest_report', 'medical_billing', 'medical_report', 'criminal_record',\n",
    "                 'floor_plan', 'site_plan', 'building_plan', 'building_code', 'code_enforcement', 'personnel_file']\n",
    "\n",
    "data['common_bigrams'] = data['bigrams'].apply(lambda x: [i for i in x if i in common_bigrams])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine columns containing cleaned words (mash) and meaningful phrases (common_bigrams) to yield final set of words for analysis for each PRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['final_mash'] = data['mash'] + data['common_bigrams']\n",
    "\n",
    "# Remove empty lists\n",
    "data['mash_len'] = data['final_mash'].apply(lambda x: len(x))\n",
    "data = data[data['mash_len'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see the result of the final cleaned data below. \n",
    "The final_mash column represents the set of words that will be considered in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Summary</th>\n",
       "      <th>city</th>\n",
       "      <th>month_year</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>mash</th>\n",
       "      <th>mash_len</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>common_bigrams</th>\n",
       "      <th>final_mash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>We are working with an engineering firm on an upcoming project  They have asked us to gather maps for this project  Would you be able to assist me in gathering maps records as builds for any underground water facilities you may have  Something just showing the route of the water lines would do\\n\\nth ST NE to nd Ave NE Arlington Cascade Surveying  Engineering \\n\\nI have attached the scope for your convenience  Please let me know if you have questions</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "      <td>[are, working, with, an, engineering, firm, on, an, upcoming, project, they, have, asked, us, to, gather, maps, for, this, project, would, you, be, able, to, assist, me, in, gathering, maps, records, as, builds, for, any, underground, water, facilities, you, may, have, something, just, showing, the, route, of, the, water, lines, would, do, th, st, ne, to, nd, ne, cascade, surveying, engineering, i, have, attached, the, scope, for, your, convenience, please, let, me, know, if, you, have, questions]</td>\n",
       "      <td>[(are, VBP), (working, VBG), (with, IN), (an, DT), (engineering, NN), (firm, NN), (on, IN), (an, DT), (upcoming, JJ), (project, NN), (they, PRP), (have, VBP), (asked, VBN), (us, PRP), (to, TO), (gather, VB), (maps, NNS), (for, IN), (this, DT), (project, NN), (would, MD), (you, PRP), (be, VB), (able, JJ), (to, TO), (assist, VB), (me, PRP), (in, IN), (gathering, VBG), (maps, NNS), (records, NNS), (as, IN), (builds, NNS), (for, IN), (any, DT), (underground, JJ), (water, NN), (facilities, NNS), (you, PRP), (may, MD), (have, VB), (something, NN), (just, RB), (showing, VBG), (the, DT), (route, NN), (of, IN), (the, DT), (water, NN), (lines, NNS), (would, MD), (do, VB), (th, VB), (st, VB), (ne, JJ), (to, TO), (nd, VB), (ne, JJ), (cascade, NN), (surveying, VBG), (engineering, NN), (i, NN), (have, VBP), (attached, VBN), (the, DT), (scope, NN), (for, IN), (your, PRP$), (convenience, NN), (please, NN), (let, VB), (me, PRP), (know, VB), (if, IN), (you, PRP), (have, VBP), (questions, NNS)]</td>\n",
       "      <td>[work, engineering, firm, upcoming, project, gather, map, project, assist, gather, map, build, underground, water, facility, something, show, route, water, line, cascade, survey, engineering, scope, convenience]</td>\n",
       "      <td>25</td>\n",
       "      <td>[work_engineering, engineering_firm, firm_upcoming, upcoming_project, project_gather, gather_map, map_project, project_would, would_assist, assist_gather, gather_map, map_record, record_build, build_underground, underground_water, water_facility, facility_something, something_show, show_route, route_water, water_line, line_would, would_cascade, cascade_survey, survey_engineering, engineering_attach, attach_scope, scope_convenience, convenience_please, please_know]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[work, engineering, firm, upcoming, project, gather, map, project, assist, gather, map, build, underground, water, facility, something, show, route, water, line, cascade, survey, engineering, scope, convenience]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Need copies of contracts and all related documents pertaining to Topcub Aircraft property located at  th DR NE Arlington WA  between Arlington Airport Topcub Aircraft City of Arlington HCI Steel Buildings and PUD</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "      <td>[copies, of, contracts, and, all, related, documents, pertaining, to, topcub, aircraft, property, located, at, th, dr, ne, wa, between, airport, topcub, aircraft, of, hci, buildings, and, pud]</td>\n",
       "      <td>[(copies, NNS), (of, IN), (contracts, NNS), (and, CC), (all, DT), (related, JJ), (documents, NNS), (pertaining, VBG), (to, TO), (topcub, VB), (aircraft, NN), (property, NN), (located, VBN), (at, IN), (th, NN), (dr, NN), (ne, JJ), (wa, NN), (between, IN), (airport, NN), (topcub, NN), (aircraft, NN), (of, IN), (hci, NN), (buildings, NNS), (and, CC), (pud, NN)]</td>\n",
       "      <td>[contract, related, topcub, aircraft, property, locate, airport, topcub, aircraft, hci, building, pud]</td>\n",
       "      <td>13</td>\n",
       "      <td>[copy_contract, contract_related, related_document, document_pertain, pertain_topcub, topcub_aircraft, aircraft_property, property_locate, locate_airport, airport_topcub, topcub_aircraft, aircraft_hci, hci_building, building_pud]</td>\n",
       "      <td>[property_locate]</td>\n",
       "      <td>[contract, related, topcub, aircraft, property, locate, airport, topcub, aircraft, hci, building, pud, property_locate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Copies of Building Permits of  valuation and up  min for Re Roofs  min for Cell Tower upgrades Electrical Mechanical  Plumbing at  min and Solar Panels Swimming Pools  Foundations at any valuation</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "      <td>[copies, of, building, permits, of, valuation, and, up, min, for, roofs, min, for, upgrades, electrical, mechanical, plumbing, at, min, and, panels, swimming, pools, foundations, at, any, valuation]</td>\n",
       "      <td>[(copies, NNS), (of, IN), (building, VBG), (permits, NNS), (of, IN), (valuation, NN), (and, CC), (up, RB), (min, NN), (for, IN), (roofs, NN), (min, NN), (for, IN), (upgrades, JJ), (electrical, JJ), (mechanical, JJ), (plumbing, NN), (at, IN), (min, NN), (and, CC), (panels, NNS), (swimming, VBG), (pools, JJ), (foundations, NNS), (at, IN), (any, DT), (valuation, NN)]</td>\n",
       "      <td>[build, permit, valuation, min, roof, min, upgrades, electrical, mechanical, plumbing, min, panel, swim, pools, foundation, valuation]</td>\n",
       "      <td>16</td>\n",
       "      <td>[copy_build, build_permit, permit_valuation, valuation_min, min_roof, roof_min, min_upgrades, upgrades_electrical, electrical_mechanical, mechanical_plumbing, plumbing_min, min_panel, panel_swim, swim_pools, pools_foundation, foundation_valuation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[build, permit, valuation, min, roof, min, upgrades, electrical, mechanical, plumbing, min, panel, swim, pools, foundation, valuation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>police report filed to an officer against Wayne Parris DOB    from Brittany J Parris The paperwork I have has a case number D  it is also stamped at the bottom with   Iím not sure which number you will need If there is any other information needed please let me know</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "      <td>[police, report, filed, to, an, officer, against, dob, from, j, paperwork, i, have, has, a, case, number, d, it, is, also, stamped, at, the, bottom, with, iím, not, sure, which, number, you, will, need, if, there, is, any, other, information, needed, please, let, me, know]</td>\n",
       "      <td>[(police, NNS), (report, NN), (filed, VBD), (to, TO), (an, DT), (officer, NN), (against, IN), (dob, NN), (from, IN), (j, NN), (paperwork, NN), (i, NN), (have, VBP), (has, VBZ), (a, DT), (case, NN), (number, NN), (d, NN), (it, PRP), (is, VBZ), (also, RB), (stamped, VBN), (at, IN), (the, DT), (bottom, NN), (with, IN), (iím, JJ), (not, RB), (sure, JJ), (which, WDT), (number, NN), (you, PRP), (will, MD), (need, VB), (if, IN), (there, EX), (is, VBZ), (any, DT), (other, JJ), (information, NN), (needed, VBN), (please, NN), (let, VB), (me, PRP), (know, VB)]</td>\n",
       "      <td>[police, officer, dob, paperwork, case, number, stamp, bottom, iím, sure, number]</td>\n",
       "      <td>13</td>\n",
       "      <td>[police_report, report_file, file_officer, officer_dob, dob_paperwork, paperwork_case, case_number, number_stamp, stamp_bottom, bottom_iím, iím_sure, sure_number, number_need, need_information, information_need, need_please, please_know]</td>\n",
       "      <td>[police_report, case_number]</td>\n",
       "      <td>[police, officer, dob, paperwork, case, number, stamp, bottom, iím, sure, number, police_report, case_number]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Email Communications between Stephanie Shook Dave Kraski Bruce Stedman and Chad Schmidt in regards to Fire Protection District  billing and passage of contract for ALS Services \\n\\nAlso any copies of Agenda Bills D Contract and materials presented for review in Nov Dec time frame in regards to the contract</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "      <td>[email, communications, between, kraski, and, in, regards, to, protection, district, billing, and, passage, of, contract, for, als, services, also, any, copies, of, agenda, d, contract, and, materials, presented, for, review, in, time, frame, in, regards, to, the, contract]</td>\n",
       "      <td>[(email, NN), (communications, NNS), (between, IN), (kraski, NN), (and, CC), (in, IN), (regards, NNS), (to, TO), (protection, NN), (district, NN), (billing, NN), (and, CC), (passage, NN), (of, IN), (contract, NN), (for, IN), (als, NNS), (services, NNS), (also, RB), (any, DT), (copies, NNS), (of, IN), (agenda, NN), (d, NN), (contract, NN), (and, CC), (materials, NNS), (presented, VBN), (for, IN), (review, NN), (in, IN), (time, NN), (frame, NN), (in, IN), (regards, NNS), (to, TO), (the, DT), (contract, NN)]</td>\n",
       "      <td>[email, communication, kraski, protection, district, billing, passage, contract, al, service, agenda, contract, material, present, review, time, frame, contract]</td>\n",
       "      <td>18</td>\n",
       "      <td>[email_communication, communication_kraski, kraski_regard, regard_protection, protection_district, district_billing, billing_passage, passage_contract, contract_al, al_service, service_copy, copy_agenda, agenda_contract, contract_material, material_present, present_review, review_time, time_frame, frame_regard, regard_contract]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[email, communication, kraski, protection, district, billing, passage, contract, al, service, agenda, contract, material, present, review, time, frame, contract]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                                                                                                                                                                                                                                                                                                                                                                                                                                Summary       city month_year                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   token  \\\n",
       "0      0  We are working with an engineering firm on an upcoming project  They have asked us to gather maps for this project  Would you be able to assist me in gathering maps records as builds for any underground water facilities you may have  Something just showing the route of the water lines would do\\n\\nth ST NE to nd Ave NE Arlington Cascade Surveying  Engineering \\n\\nI have attached the scope for your convenience  Please let me know if you have questions  Arlington    2018-06  [are, working, with, an, engineering, firm, on, an, upcoming, project, they, have, asked, us, to, gather, maps, for, this, project, would, you, be, able, to, assist, me, in, gathering, maps, records, as, builds, for, any, underground, water, facilities, you, may, have, something, just, showing, the, route, of, the, water, lines, would, do, th, st, ne, to, nd, ne, cascade, surveying, engineering, i, have, attached, the, scope, for, your, convenience, please, let, me, know, if, you, have, questions]   \n",
       "1      1                                                                                                                                                                                                                                                   Need copies of contracts and all related documents pertaining to Topcub Aircraft property located at  th DR NE Arlington WA  between Arlington Airport Topcub Aircraft City of Arlington HCI Steel Buildings and PUD  Arlington    2018-06                                                                                                                                                                                                                                                                                                                        [copies, of, contracts, and, all, related, documents, pertaining, to, topcub, aircraft, property, located, at, th, dr, ne, wa, between, airport, topcub, aircraft, of, hci, buildings, and, pud]   \n",
       "2      2                                                                                                                                                                                                                                                                   Copies of Building Permits of  valuation and up  min for Re Roofs  min for Cell Tower upgrades Electrical Mechanical  Plumbing at  min and Solar Panels Swimming Pools  Foundations at any valuation  Arlington    2018-06                                                                                                                                                                                                                                                                                                                  [copies, of, building, permits, of, valuation, and, up, min, for, roofs, min, for, upgrades, electrical, mechanical, plumbing, at, min, and, panels, swimming, pools, foundations, at, any, valuation]   \n",
       "3      3                                                                                                                                                                                             police report filed to an officer against Wayne Parris DOB    from Brittany J Parris The paperwork I have has a case number D  it is also stamped at the bottom with   Iím not sure which number you will need If there is any other information needed please let me know  Arlington    2018-06                                                                                                                                                                                                                                       [police, report, filed, to, an, officer, against, dob, from, j, paperwork, i, have, has, a, case, number, d, it, is, also, stamped, at, the, bottom, with, iím, not, sure, which, number, you, will, need, if, there, is, any, other, information, needed, please, let, me, know]   \n",
       "4      4                                                                                                                                                    Email Communications between Stephanie Shook Dave Kraski Bruce Stedman and Chad Schmidt in regards to Fire Protection District  billing and passage of contract for ALS Services \\n\\nAlso any copies of Agenda Bills D Contract and materials presented for review in Nov Dec time frame in regards to the contract  Arlington    2018-06                                                                                                                                                                                                                                      [email, communications, between, kraski, and, in, regards, to, protection, district, billing, and, passage, of, contract, for, als, services, also, any, copies, of, agenda, d, contract, and, materials, presented, for, review, in, time, frame, in, regards, to, the, contract]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            lemma  \\\n",
       "0  [(are, VBP), (working, VBG), (with, IN), (an, DT), (engineering, NN), (firm, NN), (on, IN), (an, DT), (upcoming, JJ), (project, NN), (they, PRP), (have, VBP), (asked, VBN), (us, PRP), (to, TO), (gather, VB), (maps, NNS), (for, IN), (this, DT), (project, NN), (would, MD), (you, PRP), (be, VB), (able, JJ), (to, TO), (assist, VB), (me, PRP), (in, IN), (gathering, VBG), (maps, NNS), (records, NNS), (as, IN), (builds, NNS), (for, IN), (any, DT), (underground, JJ), (water, NN), (facilities, NNS), (you, PRP), (may, MD), (have, VB), (something, NN), (just, RB), (showing, VBG), (the, DT), (route, NN), (of, IN), (the, DT), (water, NN), (lines, NNS), (would, MD), (do, VB), (th, VB), (st, VB), (ne, JJ), (to, TO), (nd, VB), (ne, JJ), (cascade, NN), (surveying, VBG), (engineering, NN), (i, NN), (have, VBP), (attached, VBN), (the, DT), (scope, NN), (for, IN), (your, PRP$), (convenience, NN), (please, NN), (let, VB), (me, PRP), (know, VB), (if, IN), (you, PRP), (have, VBP), (questions, NNS)]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [(copies, NNS), (of, IN), (contracts, NNS), (and, CC), (all, DT), (related, JJ), (documents, NNS), (pertaining, VBG), (to, TO), (topcub, VB), (aircraft, NN), (property, NN), (located, VBN), (at, IN), (th, NN), (dr, NN), (ne, JJ), (wa, NN), (between, IN), (airport, NN), (topcub, NN), (aircraft, NN), (of, IN), (hci, NN), (buildings, NNS), (and, CC), (pud, NN)]   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [(copies, NNS), (of, IN), (building, VBG), (permits, NNS), (of, IN), (valuation, NN), (and, CC), (up, RB), (min, NN), (for, IN), (roofs, NN), (min, NN), (for, IN), (upgrades, JJ), (electrical, JJ), (mechanical, JJ), (plumbing, NN), (at, IN), (min, NN), (and, CC), (panels, NNS), (swimming, VBG), (pools, JJ), (foundations, NNS), (at, IN), (any, DT), (valuation, NN)]   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                     [(police, NNS), (report, NN), (filed, VBD), (to, TO), (an, DT), (officer, NN), (against, IN), (dob, NN), (from, IN), (j, NN), (paperwork, NN), (i, NN), (have, VBP), (has, VBZ), (a, DT), (case, NN), (number, NN), (d, NN), (it, PRP), (is, VBZ), (also, RB), (stamped, VBN), (at, IN), (the, DT), (bottom, NN), (with, IN), (iím, JJ), (not, RB), (sure, JJ), (which, WDT), (number, NN), (you, PRP), (will, MD), (need, VB), (if, IN), (there, EX), (is, VBZ), (any, DT), (other, JJ), (information, NN), (needed, VBN), (please, NN), (let, VB), (me, PRP), (know, VB)]   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [(email, NN), (communications, NNS), (between, IN), (kraski, NN), (and, CC), (in, IN), (regards, NNS), (to, TO), (protection, NN), (district, NN), (billing, NN), (and, CC), (passage, NN), (of, IN), (contract, NN), (for, IN), (als, NNS), (services, NNS), (also, RB), (any, DT), (copies, NNS), (of, IN), (agenda, NN), (d, NN), (contract, NN), (and, CC), (materials, NNS), (presented, VBN), (for, IN), (review, NN), (in, IN), (time, NN), (frame, NN), (in, IN), (regards, NNS), (to, TO), (the, DT), (contract, NN)]   \n",
       "\n",
       "                                                                                                                                                                                                                  mash  mash_len                                                                                                                                                                                                                                                                                                                                                                                                                                                                               bigrams                common_bigrams                                                                                                                                                                                                           final_mash  \n",
       "0  [work, engineering, firm, upcoming, project, gather, map, project, assist, gather, map, build, underground, water, facility, something, show, route, water, line, cascade, survey, engineering, scope, convenience]        25  [work_engineering, engineering_firm, firm_upcoming, upcoming_project, project_gather, gather_map, map_project, project_would, would_assist, assist_gather, gather_map, map_record, record_build, build_underground, underground_water, water_facility, facility_something, something_show, show_route, route_water, water_line, line_would, would_cascade, cascade_survey, survey_engineering, engineering_attach, attach_scope, scope_convenience, convenience_please, please_know]                            []  [work, engineering, firm, upcoming, project, gather, map, project, assist, gather, map, build, underground, water, facility, something, show, route, water, line, cascade, survey, engineering, scope, convenience]  \n",
       "1                                                                                                               [contract, related, topcub, aircraft, property, locate, airport, topcub, aircraft, hci, building, pud]        13                                                                                                                                                                                                                                                 [copy_contract, contract_related, related_document, document_pertain, pertain_topcub, topcub_aircraft, aircraft_property, property_locate, locate_airport, airport_topcub, topcub_aircraft, aircraft_hci, hci_building, building_pud]             [property_locate]                                                                                              [contract, related, topcub, aircraft, property, locate, airport, topcub, aircraft, hci, building, pud, property_locate]  \n",
       "2                                                                               [build, permit, valuation, min, roof, min, upgrades, electrical, mechanical, plumbing, min, panel, swim, pools, foundation, valuation]        16                                                                                                                                                                                                                               [copy_build, build_permit, permit_valuation, valuation_min, min_roof, roof_min, min_upgrades, upgrades_electrical, electrical_mechanical, mechanical_plumbing, plumbing_min, min_panel, panel_swim, swim_pools, pools_foundation, foundation_valuation]                            []                                                                               [build, permit, valuation, min, roof, min, upgrades, electrical, mechanical, plumbing, min, panel, swim, pools, foundation, valuation]  \n",
       "3                                                                                                                                    [police, officer, dob, paperwork, case, number, stamp, bottom, iím, sure, number]        13                                                                                                                                                                                                                                         [police_report, report_file, file_officer, officer_dob, dob_paperwork, paperwork_case, case_number, number_stamp, stamp_bottom, bottom_iím, iím_sure, sure_number, number_need, need_information, information_need, need_please, please_know]  [police_report, case_number]                                                                                                        [police, officer, dob, paperwork, case, number, stamp, bottom, iím, sure, number, police_report, case_number]  \n",
       "4                                                    [email, communication, kraski, protection, district, billing, passage, contract, al, service, agenda, contract, material, present, review, time, frame, contract]        18                                                                                                                                             [email_communication, communication_kraski, kraski_regard, regard_protection, protection_district, district_billing, billing_passage, passage_contract, contract_al, al_service, service_copy, copy_agenda, agenda_contract, contract_material, material_present, present_review, review_time, time_frame, frame_regard, regard_contract]                            []                                                    [email, communication, kraski, protection, district, billing, passage, contract, al, service, agenda, contract, material, present, review, time, frame, contract]  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning, we have 89,145 observations (this means that just over 10,000 PRRs contained no information that could be used for analysis. For example, a PRR that is just a person's name or an alphanumeric case number). We see below that the average PRR has just over 13 words in the final mash. It is worth noting that there are a significant number of PRRs that yield a final mash of only 1 word. As we test various LDA models, we will test a number of different additional criteria to exclude records with limited information from analysis to determine whether the performance of our models improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    89145.000000\n",
       "mean        13.144114\n",
       "std         24.484836\n",
       "min          1.000000\n",
       "25%          3.000000\n",
       "50%          6.000000\n",
       "75%         15.000000\n",
       "max       2512.000000\n",
       "Name: mash_len, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['mash_len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there is significant variation in the average request length per city. \n",
    "In some cases, cities with short average length represent cities where the provided data represented a summary of the original request (Oklahoma City) though in other cases, like Dayton, we received the raw data and the average length is still considerably shorter than other cities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city\n",
       "Arlington          11.429670\n",
       "Bainbridge         17.887482\n",
       "Boulder            23.303030\n",
       "CathedralCity      12.507463\n",
       "Clearwater          7.233235\n",
       "Dayton              2.617308\n",
       "Denton             20.767620\n",
       "Everett            13.735452\n",
       "FortCollins        64.732143\n",
       "Greensboro          2.329954\n",
       "Hayward            32.089041\n",
       "Kirkland           10.635076\n",
       "LasCruces          26.474926\n",
       "Lynnwood           16.546875\n",
       "Mercer             16.500000\n",
       "Miami              31.863489\n",
       "Middleborough      26.000000\n",
       "Nola               27.552862\n",
       "OKC                 3.289256\n",
       "Oakland            16.951460\n",
       "Olympia            11.122305\n",
       "PaloAlto           31.666667\n",
       "Peoria              7.336830\n",
       "Pullman             8.857691\n",
       "RanchoCucamonga    22.843750\n",
       "Redmond            12.497893\n",
       "Renton             18.460102\n",
       "Sacramento          4.783000\n",
       "SanFrancisco       25.152969\n",
       "Tukwila             6.114954\n",
       "Vallejo            35.956268\n",
       "WestSacramento     21.626126\n",
       "Winchester         23.638418\n",
       "Name: mash_len, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gp = data.groupby('city').mean()\n",
    "data_gp['mash_len']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see a couple of examples of the cleaned mash and the original request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164    Police Traffic Collision Report and any other police reports records or documents relating to \\nCase     \\n\\nUnit    Carla Jaramillo\\nUnit    Jordan Boss\\n\\nTime of incident   \n",
       "Name: Summary, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Summary'][data.index == 164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164    [traffic, collision, police, unit, unit, time, incident, collision_report, police_report]\n",
       "Name: final_mash, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['final_mash'][data.index == 164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000    [display, bidder, individual, line, item, bid, price, youth, job, award, bid, proposal]\n",
       "Name: final_mash, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['final_mash'][data.index == 60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would like a copy of\\xa0the records displaying the bidders and their individual line item bid prices for the Youth Study Center Streets job that was awarded bid proposal no C  Thank you'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Summary\"][60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cleaned data to csv to use for testing different LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tested a number of different parameters for the LDA models to identify the optimal model. Because these models are very computationally intensive and take a long time to run, we have included the tests and final model in a [separate notebook](https://github.com/sunlightpolicy/Sunlight_FOIA/blob/master/src/analysis/LDA_Model_Tests.ipynb). Below, we conduct the analysis on our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We write a few functions that will help us to analyze the results of our winning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highest_topic(fp):\n",
    "    # Identifies the topic that has received the highest composition score for each PRR \n",
    "    df = pd.read_csv(fp)\n",
    "    df['topic_comp'] =  df['topic_comp'].apply(lambda x:  ast.literal_eval(x))\n",
    "    df['comp_len'] = df['topic_comp'].apply(len)\n",
    "    df = df[df['comp_len'] > 0]\n",
    "    df['top_topic'] = df['topic_comp'].apply(lambda x: max(x, key=lambda item:item[1])[0])\n",
    "    df['top_topic_comp'] = df['topic_comp'].apply(lambda x: max(x, key=lambda item:item[1])[1])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topics_to_csv(df, num_topics):\n",
    "    # Creates a csv file with the PRRs that have been assigned to each highest topic in the number of topics given \n",
    "    for topic in range(0, num_topics):\n",
    "        subset = df[df['top_topic'] == topic]\n",
    "        file_name = 'topics/{}_PRR_topic_{}.csv'.format(num_topics, topic)\n",
    "        subset.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_csv(model_list):\n",
    "    # Assigns highest topics and creates individual topic csv files\n",
    "    for model in model_list:\n",
    "        fp = 'topics/lda_{}_45_topics.csv'.format(model)\n",
    "        df = pd.read_csv(fp)\n",
    "        highest_topic(df, fp)\n",
    "        topics_to_csv(df, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Best Model and Data\n",
    "\n",
    "Now that we have identified the best model to categorize the PRRs in our sample into coherent topics, we load the best model and view the 60 topics that the model has generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('complaint', 0.1985935),\n",
       "   ('log', 0.14190955),\n",
       "   ('phone', 0.08581352),\n",
       "   ('search', 0.083066285),\n",
       "   ('warrant', 0.03568498),\n",
       "   ('estate', 0.032001145),\n",
       "   ('involved', 0.026251603),\n",
       "   ('quality', 0.02599625),\n",
       "   ('real', 0.025753625),\n",
       "   ('ensure', 0.019929796)]),\n",
       " (1,\n",
       "  [('associate', 0.14829391),\n",
       "   ('client', 0.11122169),\n",
       "   ('photograph', 0.10852045),\n",
       "   ('office', 0.08708323),\n",
       "   ('follow', 0.061714146),\n",
       "   ('emergency', 0.042823374),\n",
       "   ('transfer', 0.033178225),\n",
       "   ('color', 0.024401158),\n",
       "   ('restaurant', 0.02243146),\n",
       "   ('time', 0.018097397)]),\n",
       " (2,\n",
       "  [('name', 0.15037733),\n",
       "   ('victim', 0.084349126),\n",
       "   ('crime', 0.082644366),\n",
       "   ('officer', 0.07667519),\n",
       "   ('domestic', 0.041578013),\n",
       "   ('violence', 0.03346935),\n",
       "   ('injury', 0.030954132),\n",
       "   ('capitol', 0.030555379),\n",
       "   ('location', 0.027925178),\n",
       "   ('type', 0.021750417)]),\n",
       " (3,\n",
       "  [('permit', 0.2758547),\n",
       "   ('issue', 0.11701758),\n",
       "   ('building', 0.07663683),\n",
       "   ('build', 0.06414055),\n",
       "   ('building_permit', 0.059510738),\n",
       "   ('ref', 0.056181964),\n",
       "   ('construction', 0.03771535),\n",
       "   ('without', 0.024624562),\n",
       "   ('administration', 0.020187119),\n",
       "   ('since', 0.02015884)]),\n",
       " (4,\n",
       "  [('list', 0.24116392),\n",
       "   ('service', 0.123515554),\n",
       "   ('well', 0.072589695),\n",
       "   ('within', 0.069015294),\n",
       "   ('person', 0.06510219),\n",
       "   ('month', 0.06098719),\n",
       "   ('law', 0.03411746),\n",
       "   ('enforcement', 0.032960806),\n",
       "   ('residence', 0.028089434),\n",
       "   ('last', 0.026984973)]),\n",
       " (5,\n",
       "  [('show', 0.10214498),\n",
       "   ('use', 0.08015429),\n",
       "   ('policy', 0.058283392),\n",
       "   ('maintenance', 0.039954595),\n",
       "   ('year', 0.03847074),\n",
       "   ('procedure', 0.037714776),\n",
       "   ('parkway', 0.036550127),\n",
       "   ('today', 0.02697065),\n",
       "   ('operation', 0.025670737),\n",
       "   ('old', 0.024925187)]),\n",
       " (6,\n",
       "  [('avenue', 0.35911864),\n",
       "   ('drive', 0.17917691),\n",
       "   ('account', 0.054597758),\n",
       "   ('return', 0.032051444),\n",
       "   ('son', 0.024467614),\n",
       "   ('cell', 0.023386981),\n",
       "   ('wonder', 0.019287072),\n",
       "   ('geotechnical', 0.016777175),\n",
       "   ('generate', 0.015776062),\n",
       "   ('pole', 0.010756759)]),\n",
       " (7,\n",
       "  [('home', 0.07116078),\n",
       "   ('park', 0.063897714),\n",
       "   ('house', 0.056650896),\n",
       "   ('lot', 0.049521584),\n",
       "   ('place', 0.048671838),\n",
       "   ('back', 0.04694188),\n",
       "   ('tree', 0.041362315),\n",
       "   ('area', 0.039702497),\n",
       "   ('residential', 0.029248808),\n",
       "   ('land', 0.02735575)]),\n",
       " (8,\n",
       "  [('order', 0.06940024),\n",
       "   ('item', 0.05680285),\n",
       "   ('invoice', 0.04215782),\n",
       "   ('detail', 0.03767509),\n",
       "   ('purchase', 0.03386113),\n",
       "   ('number', 0.029808844),\n",
       "   ('sale', 0.029691221),\n",
       "   ('several', 0.022504056),\n",
       "   ('http', 0.018583613),\n",
       "   ('commission', 0.018436963)]),\n",
       " (9,\n",
       "  [('local', 0.10987223),\n",
       "   ('international', 0.038030066),\n",
       "   ('live', 0.03651968),\n",
       "   ('builts', 0.035503764),\n",
       "   ('range', 0.033208173),\n",
       "   ('move', 0.027478758),\n",
       "   ('contractor', 0.025758244),\n",
       "   ('print', 0.023395741),\n",
       "   ('approx', 0.021365745),\n",
       "   ('shoot', 0.020594789)]),\n",
       " (10,\n",
       "  [('arrest', 0.22508794),\n",
       "   ('notice', 0.10653793),\n",
       "   ('citation', 0.093008235),\n",
       "   ('come', 0.054579236),\n",
       "   ('say', 0.054288175),\n",
       "   ('arrest_report', 0.030788284),\n",
       "   ('accept', 0.023258643),\n",
       "   ('revocation', 0.01854626),\n",
       "   ('recently', 0.017429836),\n",
       "   ('officer', 0.016972534)]),\n",
       " (11,\n",
       "  [('contract', 0.12024548),\n",
       "   ('bid', 0.08086701),\n",
       "   ('submit', 0.0620963),\n",
       "   ('ordinance', 0.048248757),\n",
       "   ('subcontractor', 0.04418181),\n",
       "   ('service', 0.042105265),\n",
       "   ('project', 0.041909378),\n",
       "   ('rfp', 0.036090374),\n",
       "   ('approve', 0.03256977),\n",
       "   ('award', 0.031801503)]),\n",
       " (12,\n",
       "  [('camera', 0.08036688),\n",
       "   ('entity', 0.074918844),\n",
       "   ('release', 0.058930222),\n",
       "   ('intend', 0.021181582),\n",
       "   ('help', 0.021179346),\n",
       "   ('pull', 0.019310843),\n",
       "   ('attachment', 0.018467927),\n",
       "   ('expedite', 0.01831748),\n",
       "   ('appreciated', 0.0180274),\n",
       "   ('content', 0.017454458)]),\n",
       " (13,\n",
       "  [('call', 0.32521334),\n",
       "   ('note', 0.088374816),\n",
       "   ('nineoneone', 0.0701484),\n",
       "   ('cad', 0.066656),\n",
       "   ('stop', 0.04774236),\n",
       "   ('frame', 0.034719706),\n",
       "   ('dispatch', 0.03386697),\n",
       "   ('turn', 0.019149732),\n",
       "   ('time', 0.017925613),\n",
       "   ('recording', 0.016269712)]),\n",
       " (14,\n",
       "  [('communication', 0.0688568),\n",
       "   ('correspondence', 0.06803626),\n",
       "   ('staff', 0.04231121),\n",
       "   ('meeting', 0.04045327),\n",
       "   ('concern', 0.033581033),\n",
       "   ('member', 0.033549283),\n",
       "   ('note', 0.02915484),\n",
       "   ('official', 0.028396739),\n",
       "   ('limited', 0.023606328),\n",
       "   ('minute', 0.020674579)]),\n",
       " (15,\n",
       "  [('tax', 0.10910112),\n",
       "   ('amount', 0.062115442),\n",
       "   ('check', 0.047044717),\n",
       "   ('payee', 0.028844092),\n",
       "   ('employment', 0.023935616),\n",
       "   ('performance', 0.02222605),\n",
       "   ('initial', 0.021986058),\n",
       "   ('great', 0.021605315),\n",
       "   ('outstanding', 0.020619854),\n",
       "   ('possible', 0.02048308)]),\n",
       " (16,\n",
       "  [('hit', 0.091981694),\n",
       "   ('mention', 0.062275108),\n",
       "   ('next', 0.06079646),\n",
       "   ('run', 0.054484926),\n",
       "   ('applicant', 0.052003473),\n",
       "   ('study', 0.049023617),\n",
       "   ('requester', 0.039278906),\n",
       "   ('support', 0.03417416),\n",
       "   ('apply', 0.029796638),\n",
       "   ('downtown', 0.025259892)]),\n",
       " (17,\n",
       "  [('map', 0.08679456),\n",
       "   ('possible', 0.057452887),\n",
       "   ('identify', 0.04500377),\n",
       "   ('free', 0.03915043),\n",
       "   ('appreciate', 0.038111392),\n",
       "   ('formal', 0.0354281),\n",
       "   ('feel', 0.034807913),\n",
       "   ('block', 0.028777761),\n",
       "   ('much', 0.026496025),\n",
       "   ('hard', 0.026162075)]),\n",
       " (18,\n",
       "  [('data', 0.1092429),\n",
       "   ('agreement', 0.0844293),\n",
       "   ('limited', 0.08090052),\n",
       "   ('body', 0.07428584),\n",
       "   ('video', 0.062064104),\n",
       "   ('cam', 0.048334192),\n",
       "   ('room', 0.043254204),\n",
       "   ('surveillance', 0.032963812),\n",
       "   ('collect', 0.032702666),\n",
       "   ('dash', 0.01876143)]),\n",
       " (19,\n",
       "  [('theft', 0.22061245),\n",
       "   ('along', 0.06716178),\n",
       "   ('auto_theft', 0.063398145),\n",
       "   ('parking', 0.062286988),\n",
       "   ('auto', 0.04646996),\n",
       "   ('pl', 0.043082036),\n",
       "   ('southcenter', 0.035162725),\n",
       "   ('lot', 0.024039797),\n",
       "   ('main', 0.020844543),\n",
       "   ('university', 0.018294595)]),\n",
       " (20,\n",
       "  [('application', 0.085657544),\n",
       "   ('license', 0.068344794),\n",
       "   ('payroll', 0.06762842),\n",
       "   ('employee', 0.053203892),\n",
       "   ('current', 0.052077144),\n",
       "   ('business', 0.0499591),\n",
       "   ('refer', 0.03578757),\n",
       "   ('number', 0.031211086),\n",
       "   ('manager', 0.026353242),\n",
       "   ('medical', 0.025645034)]),\n",
       " (21,\n",
       "  [('present', 0.22439376),\n",
       "   ('history', 0.09230829),\n",
       "   ('report_incident', 0.059092462),\n",
       "   ('fund', 0.051201187),\n",
       "   ('period', 0.050926022),\n",
       "   ('time', 0.038735375),\n",
       "   ('animal', 0.023132412),\n",
       "   ('supplemental', 0.02311513),\n",
       "   ('signal', 0.020913407),\n",
       "   ('closure', 0.018397162)]),\n",
       " (22,\n",
       "  [('vehicle', 0.16597652),\n",
       "   ('car', 0.12360624),\n",
       "   ('street', 0.08259031),\n",
       "   ('intersection', 0.06479602),\n",
       "   ('driver', 0.059679396),\n",
       "   ('involve', 0.04571497),\n",
       "   ('accident', 0.044755075),\n",
       "   ('motor', 0.031106036),\n",
       "   ('insurance', 0.02516581),\n",
       "   ('around', 0.015663475)]),\n",
       " (23,\n",
       "  [('involve', 0.16531257),\n",
       "   ('investigation', 0.12932137),\n",
       "   ('child', 0.042590715),\n",
       "   ('usts', 0.03794015),\n",
       "   ('division', 0.031155419),\n",
       "   ('unresolved', 0.027624652),\n",
       "   ('service', 0.02494772),\n",
       "   ('training', 0.02422004),\n",
       "   ('asts', 0.022617245),\n",
       "   ('similar', 0.01915818)]),\n",
       " (24,\n",
       "  [('fire', 0.14456978),\n",
       "   ('site_assessment', 0.09028297),\n",
       "   ('environmental_site', 0.08902072),\n",
       "   ('phase_environmental', 0.06846374),\n",
       "   ('enforcement', 0.06332058),\n",
       "   ('info', 0.04320781),\n",
       "   ('light', 0.03646397),\n",
       "   ('scene', 0.035504606),\n",
       "   ('side', 0.033469994),\n",
       "   ('dept', 0.02751706)]),\n",
       " (25,\n",
       "  [('collision', 0.16592814),\n",
       "   ('occur', 0.14634575),\n",
       "   ('collision_report', 0.088421896),\n",
       "   ('cannabis', 0.0380842),\n",
       "   ('event', 0.031146232),\n",
       "   ('memoranda', 0.023629144),\n",
       "   ('addition', 0.023382638),\n",
       "   ('enterprise', 0.021961),\n",
       "   ('good', 0.020487789),\n",
       "   ('attempt', 0.018331006)]),\n",
       " (26,\n",
       "  [('email', 0.05471243),\n",
       "   ('send', 0.044262353),\n",
       "   ('letter', 0.0327627),\n",
       "   ('fee', 0.025692467),\n",
       "   ('disclosure', 0.025541063),\n",
       "   ('cost', 0.02162389),\n",
       "   ('mail', 0.021287313),\n",
       "   ('following', 0.020823436),\n",
       "   ('electronic', 0.01912893),\n",
       "   ('time', 0.01801677)]),\n",
       " (27,\n",
       "  [('demand', 0.09400265),\n",
       "   ('past', 0.08884601),\n",
       "   ('community', 0.076444395),\n",
       "   ('activity', 0.06819106),\n",
       "   ('regulation', 0.06323379),\n",
       "   ('year', 0.046756167),\n",
       "   ('chapter', 0.03797209),\n",
       "   ('transcript', 0.03222767),\n",
       "   ('every', 0.028584126),\n",
       "   ('track', 0.026003918)]),\n",
       " (28,\n",
       "  [('case', 0.37224895),\n",
       "   ('number', 0.20965174),\n",
       "   ('case_number', 0.046436027),\n",
       "   ('steal', 0.043556415),\n",
       "   ('report_case', 0.026024856),\n",
       "   ('update', 0.019941686),\n",
       "   ('analysis', 0.019081214),\n",
       "   ('disposition', 0.010940141),\n",
       "   ('view', 0.010393641),\n",
       "   ('stolen', 0.009620244)]),\n",
       " (29,\n",
       "  [('property', 0.24129005),\n",
       "   ('owner', 0.09174423),\n",
       "   ('address', 0.05381202),\n",
       "   ('lien', 0.05202367),\n",
       "   ('research', 0.039118163),\n",
       "   ('due', 0.029557053),\n",
       "   ('payoff', 0.025631377),\n",
       "   ('amount', 0.025016405),\n",
       "   ('housing', 0.022862263),\n",
       "   ('sidewalk', 0.022836013)]),\n",
       " (30,\n",
       "  [('violation', 0.12519231),\n",
       "   ('code', 0.08656456),\n",
       "   ('property', 0.06848214),\n",
       "   ('open', 0.06713956),\n",
       "   ('code_violation', 0.057894886),\n",
       "   ('zone', 0.04222084),\n",
       "   ('building', 0.03874377),\n",
       "   ('fire', 0.036294296),\n",
       "   ('unit', 0.030973408),\n",
       "   ('apn', 0.030083403)]),\n",
       " (31,\n",
       "  [('video', 0.25615332),\n",
       "   ('traffic', 0.16091378),\n",
       "   ('audio', 0.11414809),\n",
       "   ('recording', 0.058318987),\n",
       "   ('window', 0.013530725),\n",
       "   ('passenger', 0.01229218),\n",
       "   ('inside', 0.01213723),\n",
       "   ('processing', 0.011798941),\n",
       "   ('photographs', 0.011124628),\n",
       "   ('bac', 0.011021805)]),\n",
       " (32,\n",
       "  [('llc', 0.07916555),\n",
       "   ('sign', 0.066731155),\n",
       "   ('district', 0.055866495),\n",
       "   ('sheet', 0.046132334),\n",
       "   ('hearing', 0.043102253),\n",
       "   ('prevent', 0.038710307),\n",
       "   ('term', 0.035818845),\n",
       "   ('connection', 0.029579524),\n",
       "   ('operate', 0.023355097),\n",
       "   ('rental', 0.023327863)]),\n",
       " (33,\n",
       "  [('accident', 0.39958552),\n",
       "   ('accident_report', 0.18422489),\n",
       "   ('auto_accident', 0.114594795),\n",
       "   ('auto', 0.09796612),\n",
       "   ('pedestrian', 0.019592816),\n",
       "   ('architectural', 0.014912473),\n",
       "   ('usaa', 0.010008935),\n",
       "   ('everything', 0.009694831),\n",
       "   ('fault', 0.006688628),\n",
       "   ('instruction', 0.006613127)]),\n",
       " (34,\n",
       "  [('location', 0.21679133),\n",
       "   ('claim', 0.09818342),\n",
       "   ('state', 0.0697553),\n",
       "   ('burglary', 0.06911095),\n",
       "   ('insured', 0.056019105),\n",
       "   ('loss', 0.027791245),\n",
       "   ('insure', 0.025631163),\n",
       "   ('measure', 0.024490217),\n",
       "   ('farm', 0.022929706),\n",
       "   ('state_farm', 0.022389304)]),\n",
       " (35,\n",
       "  [('email', 0.26834995),\n",
       "   ('id', 0.1306745),\n",
       "   ('complete', 0.08180797),\n",
       "   ('social', 0.06045802),\n",
       "   ('post', 0.036991253),\n",
       "   ('credit', 0.036454022),\n",
       "   ('approved', 0.030344106),\n",
       "   ('medium', 0.025660833),\n",
       "   ('exceed', 0.023591131),\n",
       "   ('sam', 0.013001699)]),\n",
       " (36,\n",
       "  [('tell', 0.05282206),\n",
       "   ('store', 0.048417732),\n",
       "   ('leave', 0.046139732),\n",
       "   ('action', 0.035686594),\n",
       "   ('door', 0.027427835),\n",
       "   ('unknown', 0.026335958),\n",
       "   ('speak', 0.023881935),\n",
       "   ('officer', 0.023780545),\n",
       "   ('hwy', 0.021363195),\n",
       "   ('enter', 0.021137886)]),\n",
       " (37,\n",
       "  [('year', 0.15321907),\n",
       "   ('party', 0.100014806),\n",
       "   ('last', 0.07572979),\n",
       "   ('apartment', 0.036389094),\n",
       "   ('even', 0.030202717),\n",
       "   ('happen', 0.028766967),\n",
       "   ('sure', 0.028110817),\n",
       "   ('possession', 0.027636135),\n",
       "   ('investigate', 0.025434613),\n",
       "   ('additional', 0.023297438)]),\n",
       " (38,\n",
       "  [('address', 0.37964672),\n",
       "   ('contact', 0.07585882),\n",
       "   ('charge', 0.06375808),\n",
       "   ('name', 0.051895127),\n",
       "   ('jurisdiction', 0.043860443),\n",
       "   ('response', 0.041658256),\n",
       "   ('commercial', 0.039081343),\n",
       "   ('following', 0.031024534),\n",
       "   ('assistance', 0.029206535),\n",
       "   ('advance', 0.02876223)]),\n",
       " (39,\n",
       "  [('work', 0.053015515),\n",
       "   ('pay', 0.038354263),\n",
       "   ('line', 0.030709907),\n",
       "   ('review', 0.030431146),\n",
       "   ('employee', 0.029498309),\n",
       "   ('benefit', 0.02925975),\n",
       "   ('exist', 0.025858663),\n",
       "   ('recent', 0.024774328),\n",
       "   ('total', 0.024744458),\n",
       "   ('determine', 0.023626313)]),\n",
       " (40,\n",
       "  [('photo', 0.22963522),\n",
       "   ('offense', 0.06913104),\n",
       "   ('duo', 0.047197696),\n",
       "   ('video', 0.02510717),\n",
       "   ('packet', 0.02099043),\n",
       "   ('detain', 0.020919355),\n",
       "   ('checkpoint', 0.01420033),\n",
       "   ('mcallister', 0.010638743),\n",
       "   ('engage', 0.009663894),\n",
       "   ('utc', 0.008954173)]),\n",
       " (41,\n",
       "  [('insurance', 0.10682117),\n",
       "   ('reference', 0.09845432),\n",
       "   ('type', 0.087784335),\n",
       "   ('number', 0.07008757),\n",
       "   ('location', 0.06628888),\n",
       "   ('transaction', 0.06523592),\n",
       "   ('insure', 0.06116695),\n",
       "   ('occurrence', 0.058346972),\n",
       "   ('occurrence_location', 0.05399189),\n",
       "   ('type_auto', 0.037329476)]),\n",
       " (42,\n",
       "  [('agency', 0.11959616),\n",
       "   ('respond', 0.11368961),\n",
       "   ('police department', 0.096744515),\n",
       "   ('drainage', 0.048156034),\n",
       "   ('ticket', 0.039263982),\n",
       "   ('id', 0.025365304),\n",
       "   ('videos', 0.025021417),\n",
       "   ('condominium', 0.01925798),\n",
       "   ('plumb', 0.019178534),\n",
       "   ('pc', 0.018561114)]),\n",
       " (43,\n",
       "  [('project', 0.076902196),\n",
       "   ('plan', 0.029102111),\n",
       "   ('management', 0.02659861),\n",
       "   ('development', 0.025086623),\n",
       "   ('section', 0.024244718),\n",
       "   ('construction', 0.023521416),\n",
       "   ('work', 0.020521961),\n",
       "   ('improvement', 0.01656264),\n",
       "   ('electrical', 0.015781369),\n",
       "   ('compliance', 0.015751444)]),\n",
       " (44,\n",
       "  [('plan', 0.11660539),\n",
       "   ('site', 0.099882215),\n",
       "   ('certificate', 0.09434634),\n",
       "   ('occupancy', 0.08399464),\n",
       "   ('certificate_occupancy', 0.07622669),\n",
       "   ('site_plan', 0.045080233),\n",
       "   ('final', 0.040189173),\n",
       "   ('permit', 0.034261044),\n",
       "   ('interested', 0.03197086),\n",
       "   ('variance', 0.025503768)]),\n",
       " (45,\n",
       "  [('dob', 0.22922716),\n",
       "   ('check', 0.07751342),\n",
       "   ('suspect', 0.061426185),\n",
       "   ('individual', 0.053063616),\n",
       "   ('avondale', 0.03744808),\n",
       "   ('background', 0.036360003),\n",
       "   ('protection', 0.028581668),\n",
       "   ('renovation', 0.026687257),\n",
       "   ('background_check', 0.023604758),\n",
       "   ('â€¢', 0.014700745)]),\n",
       " (46,\n",
       "  [('locate', 0.12590905),\n",
       "   ('building', 0.11562841),\n",
       "   ('property', 0.074399255),\n",
       "   ('inspection', 0.07156111),\n",
       "   ('property_locate', 0.060741402),\n",
       "   ('plan', 0.059727844),\n",
       "   ('parcel', 0.052651398),\n",
       "   ('drawing', 0.0375317),\n",
       "   ('permit', 0.029983826),\n",
       "   ('spill', 0.028118191)]),\n",
       " (47,\n",
       "  [('page', 0.089249946),\n",
       "   ('program', 0.08123574),\n",
       "   ('control', 0.062193304),\n",
       "   ('survey', 0.050580304),\n",
       "   ('do', 0.043303214),\n",
       "   ('summary', 0.036729675),\n",
       "   ('pdf', 0.035272434),\n",
       "   ('truck', 0.030877778),\n",
       "   ('garage', 0.02812399),\n",
       "   ('station', 0.027182342)]),\n",
       " (48,\n",
       "  [('incident', 0.52505744),\n",
       "   ('incident_report', 0.15395674),\n",
       "   ('assault', 0.055209246),\n",
       "   ('around', 0.04214052),\n",
       "   ('resident', 0.029872352),\n",
       "   ('instrument', 0.01338698),\n",
       "   ('hospital', 0.013050693),\n",
       "   ('inventory', 0.011345894),\n",
       "   ('fraud', 0.00896936),\n",
       "   ('dollar', 0.008546473)]),\n",
       " (49,\n",
       "  [('section', 0.051301472),\n",
       "   ('require', 0.033593092),\n",
       "   ('exempt', 0.030290857),\n",
       "   ('portion', 0.028061418),\n",
       "   ('exemption', 0.024064451),\n",
       "   ('right', 0.024005381),\n",
       "   ('denial', 0.022793548),\n",
       "   ('check', 0.02138788),\n",
       "   ('interest', 0.019739026),\n",
       "   ('deny', 0.019162482)]),\n",
       " (50,\n",
       "  [('give', 0.10310085),\n",
       "   ('floor', 0.048503865),\n",
       "   ('cause', 0.045723896),\n",
       "   ('personnel', 0.03707043),\n",
       "   ('source', 0.03620974),\n",
       "   ('pick', 0.03261719),\n",
       "   ('floor_plan', 0.023982473),\n",
       "   ('assign', 0.023205146),\n",
       "   ('responsible', 0.017972155),\n",
       "   ('identification', 0.015963526)]),\n",
       " (51,\n",
       "  [('matter', 0.08015114),\n",
       "   ('tenant', 0.042593118),\n",
       "   ('pre', 0.03209588),\n",
       "   ('represent', 0.030175263),\n",
       "   ('non', 0.028152522),\n",
       "   ('specification', 0.027090076),\n",
       "   ('via', 0.023603652),\n",
       "   ('appreciate', 0.023358887),\n",
       "   ('quarter', 0.020137502),\n",
       "   ('following', 0.018199366)]),\n",
       " (52,\n",
       "  [('first', 0.043509472),\n",
       "   ('security', 0.03722707),\n",
       "   ('proposal', 0.033257924),\n",
       "   ('set', 0.031995006),\n",
       "   ('purpose', 0.029597947),\n",
       "   ('related', 0.028097117),\n",
       "   ('federal', 0.027885847),\n",
       "   ('easy', 0.022838237),\n",
       "   ('card', 0.021906871),\n",
       "   ('act', 0.020320265)]),\n",
       " (53,\n",
       "  [('department', 0.46678814),\n",
       "   ('police_department', 0.14740212),\n",
       "   ('police', 0.13515571),\n",
       "   ('interview', 0.025639657),\n",
       "   ('officer', 0.016585566),\n",
       "   ('investigator', 0.014546079),\n",
       "   ('prevention', 0.012628612),\n",
       "   ('answer', 0.010397196),\n",
       "   ('cpl', 0.010000569),\n",
       "   ('discovery', 0.003616073)]),\n",
       " (54,\n",
       "  [('message', 0.049739394),\n",
       "   ('text', 0.04523615),\n",
       "   ('boulevard', 0.040963557),\n",
       "   ('another', 0.037343223),\n",
       "   ('perform', 0.03510619),\n",
       "   ('fill', 0.03369129),\n",
       "   ('lease', 0.033601668),\n",
       "   ('own', 0.033182345),\n",
       "   ('foot', 0.028789343),\n",
       "   ('space', 0.027618203)]),\n",
       " (55,\n",
       "  [('environmental', 0.048749115),\n",
       "   ('hazardous', 0.0456608),\n",
       "   ('storage', 0.043166153),\n",
       "   ('material', 0.042970687),\n",
       "   ('hazardous_material', 0.034293775),\n",
       "   ('tank', 0.034046005),\n",
       "   ('property', 0.033831228),\n",
       "   ('site', 0.03341891),\n",
       "   ('assessment', 0.028234059),\n",
       "   ('storage_tank', 0.025999954)]),\n",
       " (56,\n",
       "  [('police', 0.30953634),\n",
       "   ('police_report', 0.20703475),\n",
       "   ('copy_police', 0.105848916),\n",
       "   ('request_police', 0.036158293),\n",
       "   ('involve', 0.031712394),\n",
       "   ('try', 0.023015141),\n",
       "   ('officer', 0.019914081),\n",
       "   ('approximately', 0.015022011),\n",
       "   ('send', 0.012208854),\n",
       "   ('plate', 0.010990596)]),\n",
       " (57,\n",
       "  [('water', 0.1396025),\n",
       "   ('utility', 0.08118722),\n",
       "   ('sewer', 0.07739645),\n",
       "   ('anything', 0.06501578),\n",
       "   ('plat', 0.054275632),\n",
       "   ('fine', 0.05123444),\n",
       "   ('municipal', 0.040840622),\n",
       "   ('payment', 0.03251683),\n",
       "   ('electric', 0.029127842),\n",
       "   ('joemillgmailcom', 0.026157256)]),\n",
       " (58,\n",
       "  [('statement', 0.12197899),\n",
       "   ('certified', 0.07444108),\n",
       "   ('witness', 0.059776813),\n",
       "   ('court', 0.042062994),\n",
       "   ('witness_statement', 0.035788205),\n",
       "   ('form', 0.030713223),\n",
       "   ('officer', 0.027909148),\n",
       "   ('cooperation', 0.023561912),\n",
       "   ('criminal', 0.023481019),\n",
       "   ('write', 0.02166455)]),\n",
       " (59,\n",
       "  [('seek', 0.06251169),\n",
       "   ('damage', 0.058730982),\n",
       "   ('break', 0.05518673),\n",
       "   ('result', 0.05407579),\n",
       "   ('hold', 0.04934643),\n",
       "   ('facility', 0.04679601),\n",
       "   ('evidence', 0.045945678),\n",
       "   ('doc', 0.032376543),\n",
       "   ('night', 0.022582108),\n",
       "   ('cover', 0.022135464)])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best model grouped PRRs into 60 topics and imposed the requirement that the length of the final mash be at least \n",
    "# words long or the sum of the frequency count of all the words in the final mash be at least 2000.\n",
    "\n",
    "final_model = gensim.models.ldamodel.LdaModel.load('lda_data_c2000_3_60')\n",
    "final_fp = 'topics/lda_data_c2000_3_60.csv'\n",
    "final_model.show_topics(num_topics=60, formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the topics above are grouped based on different words that people use to describe similar information. For example, topic 19 represents PRRs requesting incident information about auto thefts and topic 2 represents PRRs requesting incident information about general crimes, with an emphasis on domestic violence. While it's interesting to understand the demand for information about different types of crime, for cities looking to make decisions about what information to release as open data, these two topics refer to the same type of data - police incident reports. Therefore, to provide our city partners with information about what data types are requested, we group each of the 60 topics generated by the model based upon the type of data that would satisfy the request. The resulting 19 data type categories are listed in the dictionary below.\n",
    "\n",
    "19,[('theft', 0.22061245),\n",
    "   ('along', 0.06716178),\n",
    "   ('auto_theft', 0.063398145),\n",
    "   ('parking', 0.062286988),\n",
    "   ('auto', 0.04646996),\n",
    "   ('pl', 0.043082036),\n",
    "   ('southcenter', 0.035162725),\n",
    "   ('lot', 0.024039797),\n",
    "   ('main', 0.020844543),\n",
    "   ('university', 0.018294595)])\n",
    "   \n",
    "2,[('name', 0.15037733),\n",
    "   ('victim', 0.084349126),\n",
    "   ('crime', 0.082644366),\n",
    "   ('officer', 0.07667519),\n",
    "   ('domestic', 0.041578013),\n",
    "   ('violence', 0.03346935),\n",
    "   ('injury', 0.030954132),\n",
    "   ('capitol', 0.030555379),\n",
    "   ('location', 0.027925178),\n",
    "   ('type', 0.021750417)]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_type_dict = {0: 'Complaints to city',\n",
    "                  1: 'Police incident report',\n",
    "                  2: 'Police incident report',\n",
    "                  3: 'Parcel records, permits, plans',\n",
    "                  4: '911/law enforcement service calls',\n",
    "                  5: 'Public works, utilities',\n",
    "                  6: 'Parcel records, permits, plans',\n",
    "                  7: 'Property liens',\n",
    "                  8: 'Purchasing records, contracts',\n",
    "                  9: 'Criminal record check',\n",
    "                  10: 'Criminal record check',\n",
    "                  11: 'Purchasing records, contracts',\n",
    "                  12: 'Criminal record check',\n",
    "                  13: '911/law enforcement service calls',\n",
    "                  14: 'City government meeting notes',\n",
    "                  15: 'Checks and deposits',\n",
    "                  16: 'Auto collision report',\n",
    "                  17: 'Parcel records, permits, plans',\n",
    "                  18: 'Crime photo and video',\n",
    "                  19: 'Police incident report',\n",
    "                  20: 'Employee benefits, payroll',\n",
    "                  21: 'Criminal record check',\n",
    "                  22: 'Auto collision report',\n",
    "                  23: 'Human services cases',\n",
    "                  24: 'Environmental assessment, hazardous materials',\n",
    "                  25: 'Auto collision report',\n",
    "                  26: 'City emails, social media posts',\n",
    "                  27: 'Uncategorized',\n",
    "                  28: 'Police incident report',\n",
    "                  29: 'Property liens',\n",
    "                  30: 'Building code violations',\n",
    "                  31: 'Crime photo and video',\n",
    "                  32: 'Uncategorized',\n",
    "                  33: 'Auto collision report',\n",
    "                  34: 'Police incident report',\n",
    "                  35: 'City emails, social media posts',\n",
    "                  36: 'Police incident report',\n",
    "                  37: 'Criminal record check',\n",
    "                  38: 'Property liens',\n",
    "                  39: 'Employee benefits, payroll',\n",
    "                  40: 'Crime photo and video',\n",
    "                  41: 'Auto collision report',\n",
    "                  42: 'Uncategorized',\n",
    "                  43: 'Parcel records, permits, plans',\n",
    "                  44: 'Parcel records, permits, plans',\n",
    "                  45: 'Criminal record check',\n",
    "                  46: 'Parcel records, permits, plans',\n",
    "                  47: 'Uncategorized',\n",
    "                  48: 'Police incident report',\n",
    "                  49: 'Criminal record check',\n",
    "                  50: 'Uncategorized',\n",
    "                  51: 'Public works, utilities',\n",
    "                  52: 'Uncategorized',\n",
    "                  53: 'Police incident report',\n",
    "                  54: 'Uncategorized',\n",
    "                  55: 'Environmental assessment, hazardous materials',\n",
    "                  56: 'Police incident report',\n",
    "                  57: 'Public works, utilities',\n",
    "                  58: 'Witness statements',\n",
    "                  59: 'Police incident report'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify the top topic for each PRR and assign each PRR to a data type based on the top topic\n",
    "\n",
    "final_data = highest_topic(final_fp)\n",
    "final_data['data_type'] = final_data['top_topic'].apply(lambda x: data_type_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data = pd.read_csv('../data/clean_data.csv')\n",
    "cs = clean_data['city_x'].str.split(\" \", expand = True)\n",
    "clean_data['city'] = cs[0]\n",
    "\n",
    "clean_data['my_city'] = clean_data['month_year']+clean_data['city']\n",
    "clean_data = clean_data[['my_city', 'policy', 'portal', 'robust_policy', 'robust_portal', 'policy_months', 'portal_months']]\n",
    "\n",
    "final_data['month_year'] = final_data['month_year']+'-01'\n",
    "final_data['my_city'] = final_data['month_year'] + final_data['city']\n",
    "final_data = final_data.merge(clean_data, how ='left', on = 'my_city')\n",
    "\n",
    "final_data.to_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Most Popular Catgories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Winner take all\" popularity metric:\n",
    "* Scoring Rules:\n",
    "    * Only the topic that composes the largest share of a document scores \"points\" for its \"Adjusted Popularity\" total.\n",
    "    * If a topic composes the largest share of that document, its \"points\" are its composition score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def winner_take_all(df, count, measure): \n",
    "    # categorize data by final model identify highest topics\n",
    "    final_df = df[[measure, 'top_topic_comp']]\n",
    "    if count:\n",
    "        topic_gp = final_df.groupby(measure).count()\n",
    "    else:\n",
    "        topic_gp = final_df.groupby(measure).sum()\n",
    "    topic_gp.reset_index(inplace = True)\n",
    "    topic_gp.rename(index=str, columns={\"top_topic_comp\": \"total_pop\"}, inplace = True)\n",
    "    topic_gp = topic_gp[[measure, 'total_pop']]\n",
    "\n",
    "    if measure == 'top_topic':\n",
    "        topic_gp.sort_values(by=['top_topic'], ascending = True)\n",
    "        # add in topic words\n",
    "        words_in_topics = [tup[1] for tup in final_model.show_topics(num_topics=60, formatted=False)] # update w/ winning model\n",
    "        topic_gp['topic'] = words_in_topics\n",
    "\n",
    "        topic_gp[[\"topic1\", \"topic2\", \"topic3\", \"topic4\", \n",
    "               \"topic5\", \"topic6\", \"topic7\", \"topic8\", \"topic9\", \"topic10\"]] = topic_gp.topic.apply(pd.Series)\n",
    "\n",
    "    topic_gp = topic_gp.sort_values(by = 'total_pop', ascending=False)\n",
    "    topic_gp.reset_index(drop = True, inplace = True)\n",
    "    return topic_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_gp_count = winner_take_all(final_data, True, 'top_topic')\n",
    "topic_gp_sum = winner_take_all(final_data, False, 'top_topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_gp_count.to_csv('topics/final_model_wta_count.csv')\n",
    "topic_gp_sum.to_csv('topics/final_model_wta_sum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wta_data_sum = winner_take_all(final_data, False, 'data_type')\n",
    "wta_data_count = winner_take_all(final_data, True, 'data_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wta_data_sum.to_csv('topics/final_model_wta_count_data.csv')\n",
    "wta_data_count.to_csv('topics/final_model_wta_sum_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_type</th>\n",
       "      <th>total_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police Incident Report</td>\n",
       "      <td>6056.105429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Auto Collision Report</td>\n",
       "      <td>5410.516739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Parcel Records, Permits, Plans</td>\n",
       "      <td>3635.726240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crime Photo and Video</td>\n",
       "      <td>3223.739908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Criminal Record Check</td>\n",
       "      <td>2514.669181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Environmental Assessment, Hazardous Materials</td>\n",
       "      <td>1631.869153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>City Emails, Social Media</td>\n",
       "      <td>1352.116572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>911/Law Enforcement Service Calls</td>\n",
       "      <td>1253.521343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Building Code Violations</td>\n",
       "      <td>1146.562057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Property Liens</td>\n",
       "      <td>968.321523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Purchasing Records, Contracts</td>\n",
       "      <td>957.365329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Public Works, Utilities</td>\n",
       "      <td>752.482663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>729.290085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Employee Benefits, Payroll</td>\n",
       "      <td>506.303160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Witness Statements</td>\n",
       "      <td>491.712728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Human Services Cases</td>\n",
       "      <td>357.749847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>City Government Meeting Notes</td>\n",
       "      <td>306.017153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Complaints to City</td>\n",
       "      <td>232.600234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Checks and Deposits</td>\n",
       "      <td>164.938380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        data_type    total_pop\n",
       "0                          Police Incident Report  6056.105429\n",
       "1                           Auto Collision Report  5410.516739\n",
       "2                  Parcel Records, Permits, Plans  3635.726240\n",
       "3                           Crime Photo and Video  3223.739908\n",
       "4                           Criminal Record Check  2514.669181\n",
       "5   Environmental Assessment, Hazardous Materials  1631.869153\n",
       "6                       City Emails, Social Media  1352.116572\n",
       "7               911/Law Enforcement Service Calls  1253.521343\n",
       "8                        Building Code Violations  1146.562057\n",
       "9                                  Property Liens   968.321523\n",
       "10                  Purchasing Records, Contracts   957.365329\n",
       "11                        Public Works, Utilities   752.482663\n",
       "12                                  Uncategorized   729.290085\n",
       "13                     Employee Benefits, Payroll   506.303160\n",
       "14                             Witness Statements   491.712728\n",
       "15                           Human Services Cases   357.749847\n",
       "16                  City Government Meeting Notes   306.017153\n",
       "17                             Complaints to City   232.600234\n",
       "18                            Checks and Deposits   164.938380"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wta_data_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Winner Take All with Thresholds\" Rules:\n",
    "\n",
    "Scoring Rules:\n",
    "* Same as \"Winner Take All\", except a winning topic must compose at least a certain threshold of a document to get any points.\n",
    "* We'll try 0.2 (low) and 0.5 (high) thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def winner_thresh(df, thresh, count, measure):\n",
    "    final_df = df[[measure, 'top_topic_comp']]\n",
    "\n",
    "\n",
    "    final_df = final_df[final_df['top_topic_comp'] >= thresh]\n",
    "    if count:\n",
    "        topic_gp = final_df.groupby(measure).count()\n",
    "    else:\n",
    "        topic_gp = final_df.groupby(measure).sum()\n",
    "        \n",
    "    topic_gp.reset_index(inplace = True)\n",
    "    topic_gp.rename(index=str, columns={\"top_topic_comp\": \"total_pop\"}, inplace = True)\n",
    "    topic_gp = topic_gp[[measure, 'total_pop']]\n",
    "\n",
    "    if measure == 'top_topic':\n",
    "        topic_gp.sort_values(by=['top_topic'], ascending = True)\n",
    "\n",
    "        # add in topic words\n",
    "        words_in_topics = [tup[1] for tup in final_model.show_topics(num_topics=60, formatted=False)] # update w/ winning model\n",
    "        topic_gp['topic'] = words_in_topics\n",
    "\n",
    "        topic_gp[[\"topic1\", \"topic2\", \"topic3\", \"topic4\", \n",
    "           \"topic5\", \"topic6\", \"topic7\", \"topic8\", \"topic9\", \"topic10\"]] = topic_gp.topic.apply(pd.Series)\n",
    "\n",
    "    topic_gp = topic_gp.sort_values(by='total_pop', ascending=False)\n",
    "    topic_gp.reset_index(drop = True, inplace = True)\n",
    "    return topic_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic2 = winner_thresh(final_data, 0.2, False, 'top_topic')\n",
    "topic5 = winner_thresh(final_data, 0.5, False, 'top_topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic2_dt = winner_thresh(final_data, 0.2, False, 'data_type')\n",
    "topic5_dt = winner_thresh(final_data, 0.5, False, 'data_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_type</th>\n",
       "      <th>total_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Police Incident Report</td>\n",
       "      <td>5826.818697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Auto Collision Report</td>\n",
       "      <td>5329.277310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Parcel Records, Permits, Plans</td>\n",
       "      <td>3359.758105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Crime Photo and Video</td>\n",
       "      <td>3175.558612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Criminal Record Check</td>\n",
       "      <td>2371.972375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Environmental Assessment, Hazardous Materials</td>\n",
       "      <td>1584.073551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>City Emails, Social Media</td>\n",
       "      <td>1221.631425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>911/Law Enforcement Service Calls</td>\n",
       "      <td>1184.330523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Building Code Violations</td>\n",
       "      <td>1118.106614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Purchasing Records, Contracts</td>\n",
       "      <td>886.408014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Property Liens</td>\n",
       "      <td>872.345198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Public Works, Utilities</td>\n",
       "      <td>693.691460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>643.132858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Witness Statements</td>\n",
       "      <td>462.303701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Employee Benefits, Payroll</td>\n",
       "      <td>445.997487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human Services Cases</td>\n",
       "      <td>331.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>City Government Meeting Notes</td>\n",
       "      <td>259.853408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Complaints to City</td>\n",
       "      <td>198.407311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Checks and Deposits</td>\n",
       "      <td>145.913653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        data_type    total_pop\n",
       "13                         Police Incident Report  5826.818697\n",
       "1                           Auto Collision Report  5329.277310\n",
       "12                 Parcel Records, Permits, Plans  3359.758105\n",
       "7                           Crime Photo and Video  3175.558612\n",
       "8                           Criminal Record Check  2371.972375\n",
       "10  Environmental Assessment, Hazardous Materials  1584.073551\n",
       "4                       City Emails, Social Media  1221.631425\n",
       "0               911/Law Enforcement Service Calls  1184.330523\n",
       "2                        Building Code Violations  1118.106614\n",
       "16                  Purchasing Records, Contracts   886.408014\n",
       "14                                 Property Liens   872.345198\n",
       "15                        Public Works, Utilities   693.691460\n",
       "17                                  Uncategorized   643.132858\n",
       "18                             Witness Statements   462.303701\n",
       "9                      Employee Benefits, Payroll   445.997487\n",
       "11                           Human Services Cases   331.358900\n",
       "5                   City Government Meeting Notes   259.853408\n",
       "6                              Complaints to City   198.407311\n",
       "3                             Checks and Deposits   145.913653"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic2_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Credit Approach\n",
    "\n",
    "* All topics assigned to a given PRR get credit for that PRR's topic composition score, provided the score is above the established threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prop_calc(df, thresh, measure, data_type_dict = data_type_dict):\n",
    "    results_dict = {}\n",
    "    for row_num in df.index:\n",
    "        for tup in df.topic_comp[row_num]: \n",
    "            if tup[1] >= thresh: \n",
    "                if measure == 'top_topic':\n",
    "                    if not tup[0] in results_dict:\n",
    "                        results_dict[tup[0]] = tup[1] \n",
    "                    else:\n",
    "                         results_dict[tup[0]] += tup[1]\n",
    "                else:\n",
    "                    if not data_type_dict[tup[0]] in results_dict:\n",
    "                        results_dict[data_type_dict[tup[0]]] = tup[1] \n",
    "                    else:\n",
    "                         results_dict[data_type_dict[tup[0]]] += tup[1]\n",
    "            else:\n",
    "                pass\n",
    "                        \n",
    "    pd_df = pd.DataFrame.from_dict(results_dict, orient = 'index')\n",
    "    pd_df.reset_index(inplace = True)\n",
    "    pd_df.rename(index = str, columns = {'index': measure, 0: 'total_score'}, inplace = True)\n",
    "    pd_df = pd_df.sort_values(by=[measure], ascending = True)\n",
    "\n",
    "    if measure == 'top_topic':\n",
    "        # add in topic words\n",
    "        words_in_topics = [tup[1] for tup in final_model.show_topics(num_topics=60, formatted=False)] # update w/ winning model\n",
    "        pd_df['topic_words'] = words_in_topics\n",
    "\n",
    "        pd_df[[\"topic1\", \"topic2\", \"topic3\", \"topic4\", \n",
    "               \"topic5\", \"topic6\", \"topic7\", \"topic8\", \"topic9\", \"topic10\"]] = pd_df.topic_words.apply(pd.Series)\n",
    "\n",
    "    \n",
    "    \n",
    "    pd_df = pd_df.sort_values(by='total_score', ascending=False)\n",
    "    pd_df.reset_index(drop = True, inplace = True)\n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop_calc_2 = prop_calc(final_data, .2, 'top_topic')\n",
    "prop_calc_5 = prop_calc(final_data, .5, 'top_topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop_calc_2.to_csv(\"topics/prop_calc_2_final_model.csv\")\n",
    "prop_calc_5.to_csv(\"topics/prop_calc_5_final_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop_calc_2_dt = prop_calc(final_data, .2, 'data_type')\n",
    "prop_calc_5_dt = prop_calc(final_data, .5, 'data_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop_calc_2_dt.to_csv(\"topics/prop_calc_2_dt_final_model.csv\")\n",
    "prop_calc_5_dt.to_csv(\"topics/prop_calc_5_dt_final_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_type</th>\n",
       "      <th>total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police Incident Report</td>\n",
       "      <td>7843.605297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Auto Collision Report</td>\n",
       "      <td>6220.267259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Parcel Records, Permits, Plans</td>\n",
       "      <td>4488.424044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crime Photo and Video</td>\n",
       "      <td>3767.362125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Criminal Record Check</td>\n",
       "      <td>3201.082004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Environmental Assessment, Hazardous Materials</td>\n",
       "      <td>1882.571488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>911/Law Enforcement Service Calls</td>\n",
       "      <td>1577.636194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>City Emails, Social Media</td>\n",
       "      <td>1525.486415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Property Liens</td>\n",
       "      <td>1424.262950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Building Code Violations</td>\n",
       "      <td>1371.284014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>1331.912141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Purchasing Records, Contracts</td>\n",
       "      <td>1057.819885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Public Works, Utilities</td>\n",
       "      <td>1033.044884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Witness Statements</td>\n",
       "      <td>734.011668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Employee Benefits, Payroll</td>\n",
       "      <td>676.740894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Human Services Cases</td>\n",
       "      <td>497.041964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>City Government Meeting Notes</td>\n",
       "      <td>340.738349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Complaints to City</td>\n",
       "      <td>250.339332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Checks and Deposits</td>\n",
       "      <td>198.390500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        data_type  total_score\n",
       "0                          Police Incident Report  7843.605297\n",
       "1                           Auto Collision Report  6220.267259\n",
       "2                  Parcel Records, Permits, Plans  4488.424044\n",
       "3                           Crime Photo and Video  3767.362125\n",
       "4                           Criminal Record Check  3201.082004\n",
       "5   Environmental Assessment, Hazardous Materials  1882.571488\n",
       "6               911/Law Enforcement Service Calls  1577.636194\n",
       "7                       City Emails, Social Media  1525.486415\n",
       "8                                  Property Liens  1424.262950\n",
       "9                        Building Code Violations  1371.284014\n",
       "10                                  Uncategorized  1331.912141\n",
       "11                  Purchasing Records, Contracts  1057.819885\n",
       "12                        Public Works, Utilities  1033.044884\n",
       "13                             Witness Statements   734.011668\n",
       "14                     Employee Benefits, Payroll   676.740894\n",
       "15                           Human Services Cases   497.041964\n",
       "16                  City Government Meeting Notes   340.738349\n",
       "17                             Complaints to City   250.339332\n",
       "18                            Checks and Deposits   198.390500"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_calc_2_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Metrics within a City/County (Dampened Popularity):\n",
    "* For each city/county, we add up total score fore each topic and then take the log of the total score. We then add up scores across each city/count.\n",
    "* For winner-take-all, only score for top topic included (provided it is above threshold)\n",
    "* For partial-credit, scores for all topics included (provided it is above threhsold\n",
    "* This is an extra control for cities with a large number of PRRs from skewing our results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_list = ['Arlington', 'Bainbridge', 'Boulder', 'CathedralCity' ,'Clearwater', 'Dayton', \n",
    "            'Denton', 'Everett', 'FortCollins', 'Greensboro', 'Hayward', 'Kirkland', 'LasCruces', 'Lynnwood',\n",
    "            'Mercer', 'Miami', 'Middleborough', 'Nola', 'Oakland', 'OKC', 'Olympia', 'PaloAlto', \n",
    "            'Peoria', 'Pullman', 'RanchoCucamonga', 'Redmond', 'Renton', 'Sacramento', 'SanFrancisco', \n",
    "            'Tukwila', 'Vallejo', 'WestSacramento', 'Winchester']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_pop(df, city_list, num_topics, thresh, winner_take_all, policy):\n",
    "    \n",
    "    if policy:\n",
    "        list_dicts_1 = []\n",
    "        list_dicts_0 = []\n",
    "        \n",
    "    else:\n",
    "        list_of_domain_dicts = []\n",
    "    \n",
    "    for city in city_list:\n",
    "        if policy:\n",
    "            results_dict_1 = {}\n",
    "            results_dict_0 = {}\n",
    "            for num in range(0, num_topics):\n",
    "                results_dict_1[num] = 0\n",
    "                results_dict_0[num] = 0\n",
    "        else:\n",
    "            results_dict = {}\n",
    "            for num in range(0, num_topics):\n",
    "                results_dict[num] = 0\n",
    "        \n",
    "        #get our df only of rows from a given city/state domain\n",
    "        city_df = df[df.city == city]\n",
    "        \n",
    "        for row_num in city_df.index:\n",
    "            tup_list = city_df.topic_comp[row_num] #list of (topic, doc composition) tuples\n",
    "            \n",
    "            if winner_take_all:\n",
    "        \n",
    "                #return only the tuple w/highest topic composition value\n",
    "                winner_tuple = max(tup_list, key=lambda item:item[1]) \n",
    "                if winner_tuple[1] > thresh:\n",
    "                    if policy:\n",
    "                        if city_df.policy[row_num] == 1:\n",
    "                            results_dict_1[winner_tuple[0]] += winner_tuple[1]\n",
    "                                \n",
    "                        else:\n",
    "                            results_dict_0[winner_tuple[0]] += winner_tuple[1]\n",
    "                        \n",
    "                        \n",
    "                    else:\n",
    "                        results_dict[winner_tuple[0]] += winner_tuple[1] \n",
    "                            \n",
    "            else:\n",
    "                if policy:\n",
    "                    if city_df.policy[row_num] == 1:\n",
    "                        for tup in tup_list: \n",
    "                            if tup[1] >= thresh:\n",
    "                                results_dict_1[tup[0]] += tup[1] \n",
    "                    else:\n",
    "                         for tup in tup_list: \n",
    "                            if tup[1] >= thresh:\n",
    "                                results_dict_0[tup[0]] += tup[1]\n",
    "                else:\n",
    "                    \n",
    "                    for tup in tup_list: \n",
    "                        if tup[1] >= thresh:\n",
    "                            results_dict[tup[0]] += tup[1] \n",
    "            \n",
    "        #when loop of domain_df is finished, take log of all keys in dict\n",
    "        if policy:\n",
    "            log_dict_1 = {}\n",
    "            for k,v in results_dict_1.items():\n",
    "                if v != 0:\n",
    "                    log_dict_1[k] = np.log(v) \n",
    "                else:\n",
    "                    log_dict_1[k] = 0\n",
    "                \n",
    "            log_dict_0 = {}\n",
    "            for k,v in results_dict_0.items():\n",
    "                if v != 0:\n",
    "                    log_dict_0[k] = np.log(v) \n",
    "                else:\n",
    "                    log_dict_0[k] = 0\n",
    "        else:\n",
    "            log_dict = {}\n",
    "            for k,v in results_dict.items():\n",
    "                if v != 0:\n",
    "                    log_dict[k] = np.log(v) \n",
    "                else:\n",
    "                    log_dict[k] = 0\n",
    "\n",
    "        if policy:\n",
    "            list_dicts_1.append(log_dict_1)\n",
    "            list_dicts_0.append(log_dict_0)\n",
    "        else:\n",
    "            #now we have a polished dict of topic numbers as keys and log of all views/DLs as values; append it to list\n",
    "            list_of_domain_dicts.append(log_dict)\n",
    "    \n",
    "    #use Counter() object to sync our dictionaries\n",
    "    if policy:\n",
    "        c1 = Counter()\n",
    "        for d1 in list_dicts_1:\n",
    "            c1.update(d1)\n",
    "        c0 = Counter()\n",
    "        for d0 in list_dicts_0:\n",
    "            c0.update(d0)\n",
    "    else:  \n",
    "        c = Counter()\n",
    "        for d in list_of_domain_dicts:\n",
    "            c.update(d)\n",
    "\n",
    "    if policy:\n",
    "        pd1 = dict(c1)\n",
    "        pd0 = dict(c0)\n",
    "    else:\n",
    "        popularity_dict = dict(c)\n",
    "    \n",
    "    if policy:\n",
    "        pd_df1 = pd.DataFrame.from_dict(pd1, orient = 'index')\n",
    "        pd_df1.reset_index(inplace = True)\n",
    "        pd_df1['policy'] = 1\n",
    "        pd_df1.rename(index = str, columns = {'index': 'topic', 0: 'total_score'}, inplace = True)\n",
    "        \n",
    "        pd_df1.topic = pd.to_numeric(pd_df1.topic)\n",
    "        pd_df1 = pd_df1.sort_values(by=['topic'], ascending = True)\n",
    "\n",
    "        # add in topic words\n",
    "        words_in_topics = [tup[1] for tup in final_model.show_topics(num_topics=60, formatted=False)] # update w/ winning model\n",
    "        pd_df1['topic_words'] = words_in_topics\n",
    "\n",
    "        pd_df1[[\"topic1\", \"topic2\", \"topic3\", \"topic4\", \n",
    "               \"topic5\", \"topic6\", \"topic7\", \"topic8\", \"topic9\", \"topic10\"]] = pd_df1.topic_words.apply(pd.Series)\n",
    "        \n",
    "        pd_df0 = pd.DataFrame.from_dict(pd0, orient = 'index')\n",
    "        pd_df0.reset_index(inplace = True)\n",
    "        pd_df0['policy'] = 0\n",
    "        pd_df0.rename(index = str, columns = {'index': 'topic', 0: 'total_score'}, inplace = True)\n",
    "        pd_df0.topic = pd.to_numeric(pd_df0.topic)\n",
    "        pd_df0 = pd_df0.sort_values(by=['topic'], ascending = True)\n",
    "\n",
    "        # add in topic words\n",
    "        words_in_topics = [tup[1] for tup in final_model.show_topics(num_topics=60, formatted=False)] # update w/ winning model\n",
    "        pd_df0['topic_words'] = words_in_topics\n",
    "\n",
    "        pd_df0[[\"topic1\", \"topic2\", \"topic3\", \"topic4\", \n",
    "               \"topic5\", \"topic6\", \"topic7\", \"topic8\", \"topic9\", \"topic10\"]] = pd_df0.topic_words.apply(pd.Series)\n",
    "    \n",
    "        \n",
    "        pd_df = pd.concat([pd_df0, pd_df1])\n",
    "    else:\n",
    "        pd_df = pd.DataFrame.from_dict(popularity_dict, orient = 'index')\n",
    "        pd_df.reset_index(inplace = True)\n",
    "    \n",
    "        pd_df.rename(index = str, columns = {'index': 'topic', 0: 'total_score'}, inplace = True)\n",
    "        pd_df.topic = pd.to_numeric(pd_df.topic)\n",
    "        pd_df = pd_df.sort_values(by=['topic'], ascending = True)\n",
    "\n",
    "        # add in topic words\n",
    "        words_in_topics = [tup[1] for tup in final_model.show_topics(num_topics=60, formatted=False)] # update w/ winning model\n",
    "        pd_df['topic_words'] = words_in_topics\n",
    "\n",
    "        pd_df[[\"topic1\", \"topic2\", \"topic3\", \"topic4\", \n",
    "               \"topic5\", \"topic6\", \"topic7\", \"topic8\", \"topic9\", \"topic10\"]] = pd_df.topic_words.apply(pd.Series)\n",
    "\n",
    "\n",
    "\n",
    "    pd_df = pd_df.sort_values(by='total_score', ascending=False)\n",
    "\n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove observations from June 2018 since we obtained data from the cities in our sample at different points throughout the month of June. Because we are looking at differences between the treatment and control groups, we do not want differences in date different cities' data were accessed to influence our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pol_df = final_data[final_data['month_year'] != '2018-06-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculates dampened popularity of data types from data frame with topic dampened popularity\n",
    "\n",
    "def np_topic_to_data(np_df, policy, data_type_dict = data_type_dict):\n",
    "    np_df['data_type'] = np_df['topic'].apply(lambda x: data_type_dict[x])\n",
    "    if policy:\n",
    "        np_gp = np_df.groupby(['data_type', 'policy']).sum()\n",
    "    else:\n",
    "        np_gp = np_df.groupby('data_type').sum()\n",
    "    np_gp.reset_index(inplace= True)\n",
    "    np_gp.sort_values(by = 'total_score', inplace = True, ascending = False)\n",
    "    np_gp.reset_index(drop = True, inplace = True)\n",
    "    np_gp.drop(columns=['topic'], inplace = True)\n",
    "    return np_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# winner take all with different thresholds for topic and data types\n",
    "np_df2 = norm_pop(pol_df, city_list, 60, .2, True, False)\n",
    "np_df5 = norm_pop(pol_df, city_list, 60, .5, True, False)\n",
    "np_df2.to_csv('topics/norm_pop_2_final.csv')\n",
    "np_df5.to_csv('topics/norm_pop_5_final.csv')\n",
    "np_data_type2 = np_topic_to_data(np_df2, False)\n",
    "np_data_type5 = np_topic_to_data(np_df5, False)\n",
    "np_data_type2.to_csv('topics/norm_pop_2_dt_final.csv')\n",
    "np_data_type5.to_csv('topics/norm_pop_5_dt_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# proportional calculation with different thresholds for topic and data types\n",
    "np_df2_prop = norm_pop(pol_df, city_list, 60, .2, False, False)\n",
    "np_df5_prop = norm_pop(pol_df, city_list, 60, .5, False, False)\n",
    "np_df2_prop.to_csv('topics/norm_pop_2_prop_final.csv')\n",
    "np_df5_prop.to_csv('topics/norm_pop_5_prop_final.csv')\n",
    "np_data_type2_prop = np_topic_to_data(np_df2_prop, False)\n",
    "np_data_type5_prop = np_topic_to_data(np_df5_prop, False)\n",
    "np_data_type2_prop.to_csv('topics/norm_pop_2_dt_prop_final.csv')\n",
    "np_data_type5_prop.to_csv('topics/norm_pop_5_dt_prop_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# difference in topics w/ policy and proportional count\n",
    "np_df2_prop = norm_pop(pol_df, city_list, 60, .2, False, True)\n",
    "np_data_type2_prop = np_topic_to_data(np_df2_prop, True)\n",
    "np_data_type2_prop.to_csv('topics/norm_pop_2_dt_prop_pol_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_policy = norm_pop(pol_df, city_list, 60, .2, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_policy = np_topic_to_data_type(dt_policy, True)\n",
    "dt_policy.to_csv('topics/dt_policy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_list_sub = ['Greensboro', 'NewOrleans', 'FortCollins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pol_df_sub = pol_df[pol_df['city'].isin(city_list_sub)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_df2_sub = norm_pop(pol_df_sub, city_list, 60, .2, True, True)\n",
    "np_data_type2_sub = np_topic_to_data(np_df2_sub, True)\n",
    "np_data_type2_sub.to_csv('topics/norm_pop_2_dt_sub_pol_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for city in city_list_sub:\n",
    "    pol_df_city = pol_df[pol_df['city'] == city]\n",
    "    np_df2_city = norm_pop(pol_df_city, [city], 60, .2, True, True)\n",
    "    np_data_type2_city = np_topic_to_data(np_df2_city, True)\n",
    "    fp = 'topics/norm_pop_2_dt_sub_pol_{}.csv'.format(city)\n",
    "    np_data_type2_city.to_csv(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Topic Popularity by City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pop_by_city(df, city_list, num_topics, thresh, winner_take_all):\n",
    "    \n",
    "    \n",
    "    cols = ['city', 'topic', 'total_pop', 'pct'] \n",
    "    \n",
    "    topic_pop_city = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for city in city_list:\n",
    "        \n",
    "        results_dict = {}\n",
    "        for i in range(0, num_topics):\n",
    "            results_dict[i] = 0\n",
    "        \n",
    "        #get our df only of rows from a given city/state domain\n",
    "        city_df = df[df.city == city]\n",
    "        \n",
    "        for row_num in city_df.index:\n",
    "            tup_list = city_df.topic_comp[row_num] #list of (topic, doc composition) tuples\n",
    "            \n",
    "            if winner_take_all:\n",
    "        \n",
    "                #return only the tuple w/highest topic composition value\n",
    "                winner_tuple = max(tup_list, key=lambda item:item[1])  \n",
    "\n",
    "                if not winner_tuple[0] in results_dict: #if not in dict, add it with its TOTAL VIEWS score\n",
    "                    if winner_tuple[1] > thresh:\n",
    "                        results_dict[winner_tuple[0]] = winner_tuple[1] \n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                if winner_tuple[0] in results_dict: #if in dict, increment that key's value with score\n",
    "                    if winner_tuple[1] > thresh:\n",
    "                        results_dict[winner_tuple[0]] += winner_tuple[1]\n",
    "                    pass\n",
    "            else:\n",
    "                for tup in tup_list: \n",
    "                    if not tup[0] in results_dict:\n",
    "                        if tup[1] >= thresh: \n",
    "                            results_dict[tup[0]] = tup[1] \n",
    "                    else:\n",
    "                        pass\n",
    "                    if tup[0] in results_dict:\n",
    "                        if tup[1] >= thresh:\n",
    "                            results_dict[tup[0]] += tup[1] \n",
    "        \n",
    "        pd_df = pd.DataFrame.from_dict(results_dict, orient = 'index')\n",
    "        pd_df.reset_index(inplace = True)\n",
    "        pd_df.rename(index = str, columns = {'index': 'topic', 0: 'total_pop'}, inplace = True)\n",
    "        pd_df['city'] = city\n",
    "        pd_df['pct'] = (pd_df['total_pop']/sum(pd_df['total_pop']))*100\n",
    "        topic_pop_city = pd.concat([topic_pop_city, pd_df])\n",
    "\n",
    "    return topic_pop_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbc = pop_by_city(final_data, city_list, 60, .2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbc['data_type'] = pbc['topic'].apply(lambda x: data_type_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbc.to_csv('pbc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbc_gb = pbc[pbc['city'] == \"Greensboro\"]\n",
    "pbc_gb.sort_values(by = 'total_pop', ascending = False, inplace = True)\n",
    "pbc_gb['topic'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_set = set()\n",
    "dt_set = set()\n",
    "\n",
    "for city in city_list:\n",
    "    pbc_city = pbc[pbc['city'] == city]\n",
    "    pbc_city.sort_values(by = 'total_pop', ascending = False, inplace = True)\n",
    "    topic_set.add(pbc_city['topic'].iloc[0])\n",
    "    fp = 'topics/topic_pop_{}.csv'.format(city)\n",
    "    pbc_city.to_csv(fp)\n",
    "    \n",
    "    pbc_city_gp = pbc_city.groupby('data_type').sum()\n",
    "    pbc_city_gp.sort_values(by = 'total_pop', ascending = False, inplace = True)\n",
    "    pbc_city_gp.reset_index(inplace=True)\n",
    "    dt_set.add(pbc_city_gp['data_type'].iloc[0])\n",
    "    fp2 = 'topics/data_type_pop_{}.csv'.format(city)\n",
    "    pbc_city_gp.to_csv(fp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dt_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City Deep-Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pol_df.to_csv('pol_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_dd = ['Clearwater','Greensboro', 'FortCollins', 'Nola']\n",
    "\n",
    "for city in city_dd:\n",
    "    city_df = final_df[final_df['city'] == city]\n",
    "    month_count = city_df.groupby(['month_year', 'top_topic']).sum()\n",
    "    month_count = month_count.reset_index()\n",
    "    month_count = month_count[['month_year', 'top_topic', 'top_topic_comp']]\n",
    "   \n",
    "    fn = '{}_topic.csv'.format(city)\n",
    "    month_count.to_csv(fn)\n",
    "    \n",
    "    df = month_count.groupby('top_topic').sum()\n",
    "    df.sort_values(by = 'top_topic_comp', ascending = False, inplace = True)\n",
    "    \n",
    "    fn_tt = '{}_top_topic.csv'.format(city)\n",
    "    df.to_csv(fn_tt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
