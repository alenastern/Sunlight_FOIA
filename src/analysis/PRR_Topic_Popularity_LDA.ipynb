{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# system tools\n",
    "import warnings\n",
    "import json\n",
    "import sys\n",
    "import string\n",
    "import ast\n",
    "\n",
    "# data cleaning + analysis tools\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#nltk tools\n",
    "import spacy\n",
    "import lda #Latent Dirichlet Allocation (create topics)\n",
    "import gensim\n",
    "from gensim import corpora, models #for constructing document term matrix\n",
    "#from stop_words import get_stop_words\n",
    "from gensim.models import Phrases\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#set notebook preferences\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import JSON file with city metadata \n",
    "\n",
    "This including which cities have published the raw Public Record Requests (PRRs) they receive for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = '../data/cities.json'\n",
    "\n",
    "with open(json_file, 'r') as f:\n",
    "    md = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create dataframe of PRR data for all relevant cities\n",
    "\n",
    "This dataframe includes PPR data from the **34 cities** in our sample (52 total cities) that had sufficient raw data with the public record request for analysis. Our sample represents cities that host an online PRR portal for submitting requests.  These data were obtained through a variety of methods including:\n",
    "\n",
    "1. exporting the full archive of PRRs hosted on the online portal as a csv file  \n",
    "2. scraping the full history of PRR data from portals which publish previous requests, but do not offer a download option (see [scraping notebook](https://github.com/sunlightpolicy/Sunlight_FOIA/blob/master/src/data/NR_Scrape.ipynb))\n",
    "3. downloading public records request data that has been published on city’s open data portal  \n",
    "4. submitting a public record request to obtain the archive of PRR data \n",
    "\n",
    "It is worth noting that *specificities of the different city portals influence the substance of the public record requests received*. For example, the city of Clearwater, FL has separate request forms for police records and public records, prompting citizens who submit police record requests to provide the specific case number. In addition, while most of the data released by cities is the raw request submitted by citizens, in a few cases the city released a summary of the submitted request prepared by city staff. For example, the Oklahoma City clerk's office released the summary of the request and the department the request was routed to for response. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arlington city\n",
      "Asheville city\n",
      "Bainbridge Island city\n",
      "Boulder County\n",
      "Cathedral City city\n",
      "Clearwater city\n",
      "Dayton city\n",
      "Denton city\n",
      "Everett city\n",
      "Fort Collins city\n",
      "Greensboro city\n",
      "Hayward city\n",
      "Kirkland city\n",
      "Las Cruces city\n",
      "Lynnwood city\n",
      "Mercer Island city\n",
      "Miami city\n",
      "Middleborough town\n",
      "New Orleans city\n",
      "Oakland city\n",
      "Oklahoma City city\n",
      "Olympia city\n",
      "Palo Alto city\n",
      "Peoria city\n",
      "Pullman city\n",
      "Rancho Cucamonga city\n",
      "Redmond city\n",
      "Renton city\n",
      "Sacramento city\n",
      "San Francisco city\n",
      "Tukwila city\n",
      "Vallejo city\n",
      "West Sacramento city\n",
      "Winchester city\n",
      "['Arlington', 'Asheville', 'Bainbridge', 'Island', 'Boulder', 'Cathedral', 'City', 'Clearwater', 'Dayton', 'Denton', 'Everett', 'Fort', 'Collins', 'Greensboro', 'Hayward', 'Kirkland', 'Las', 'Cruces', 'Lynnwood', 'Mercer', 'Island', 'Miami', 'Middleborough', 'New', 'Orleans', 'Oakland', 'Oklahoma', 'City', 'Olympia', 'Palo', 'Alto', 'Peoria', 'Pullman', 'Rancho', 'Cucamonga', 'Redmond', 'Renton', 'Sacramento', 'San', 'Francisco', 'Tukwila', 'Vallejo', 'West', 'Sacramento', 'Winchester']\n"
     ]
    }
   ],
   "source": [
    "data_raw = pd.DataFrame(columns = ['city', 'month_year', 'Summary'])\n",
    "city_list = []\n",
    "for key, value in md.items():\n",
    "    city = value['name']\n",
    "    filepath = '/Users/alenastern/Google Drive File Stream/My Drive/Alena_Project/PR_Data/{}.csv'.format(city)\n",
    "    # tag in metadata for whether city publishes request content\n",
    "    if value[\"desc\"] == \"Y\":\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding='mac_roman')\n",
    "            except:\n",
    "                continue\n",
    "        print(key)\n",
    "        name = key.split(' ')\n",
    "        city_list.extend([x for x in name[:-1]])\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df['Create Date'] = pd.to_datetime(df['Create Date'])\n",
    "    except:\n",
    "        df['New'] = pd.to_datetime(df['Create Date'].apply(lambda x: re.findall('^\\S*', x)[0]))\n",
    "        df.drop(columns=['Create Date'], inplace = True)\n",
    "        df.rename(index=str, columns={\"New\": \"Create Date\"}, inplace = True)\n",
    "\n",
    "    df['month_year'] = df['Create Date'].dt.to_period('M')\n",
    "    \n",
    "    mc = df[['month_year', 'Summary']]\n",
    "    mc['city'] = city\n",
    "    \n",
    "    data_raw = pd.concat([data_raw, mc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our raw dataset includes 110,138 PRRs from 34 different cities\n",
    "\n",
    "NEED TO EDIT BEFORE FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_raw.to_csv('data_raw.csv')\n",
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_raw.city.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6411c47fa744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create sequential numeric index for data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRangeIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# create sequential numeric index for data\n",
    "\n",
    "data_raw.index = pd.RangeIndex(len(data_raw.index))\n",
    "data_raw.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe for cleaning by removing null summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop observations that are null for the raw PRR content field ('Summary')\n",
    "data = data_raw.dropna(subset=['Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the raw data below. As we can see, the text in the Summary field is very messy and will require a lot of cleaning to prepare the data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Summary</th>\n",
       "      <th>city</th>\n",
       "      <th>month_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>We are working with an engineering firm on an upcoming project.  They have asked us to gather maps for this project.  Would you be able to assist me in gathering maps/records (as builds) for any underground water facilities you may have?  Something just showing the route of the water lines would do.\\n\\n207th ST NE to 92nd Ave NE, Arlington, Cascade Surveying &amp; Engineering \\n\\nI have attached the scope for your convenience.  Please let me know if you have questions.</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Need copies of contracts and all related documents pertaining to Topcub Aircraft property located at 17922 59th DR NE Arlington WA 98223 between Arlington Airport, Topcub Aircraft, City of Arlington, HCI Steel Buildings and PUD.</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Copies of Building Permits of $5,000 valuation and up ($20,000 min for Re-Roofs), ($50,000 min. for Cell Tower upgrades), (Electrical, Mechanical &amp; Plumbing at $100,000 min.) and (Solar Panels, Swimming Pools &amp; Foundations at any valuation)</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>police report filed to an officer against Wayne Parris (DOB 08-03-1957) from Brittany J. Parris. The paperwork I have has a case number D18-39 it is also stamped at the bottom with 18-1294, Iím not sure which number you will need. If there is any other information needed please let me know.</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Email Communications between Stephanie Shook, Dave Kraski, Bruce Stedman and Chad Schmidt in regards to Fire Protection District 21 billing and passage of contract for ALS Services. \\n\\nAlso any copies of Agenda Bills, D21 Contract and materials presented for review in Nov/Dec time frame in regards to the contract.</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Summary       city month_year\n",
       "0      0  We are working with an engineering firm on an upcoming project.  They have asked us to gather maps for this project.  Would you be able to assist me in gathering maps/records (as builds) for any underground water facilities you may have?  Something just showing the route of the water lines would do.\\n\\n207th ST NE to 92nd Ave NE, Arlington, Cascade Surveying & Engineering \\n\\nI have attached the scope for your convenience.  Please let me know if you have questions.  Arlington    2018-06\n",
       "1      1                                                                                                                                                                                                                                                   Need copies of contracts and all related documents pertaining to Topcub Aircraft property located at 17922 59th DR NE Arlington WA 98223 between Arlington Airport, Topcub Aircraft, City of Arlington, HCI Steel Buildings and PUD.  Arlington    2018-06\n",
       "2      2                                                                                                                                                                                                                                       Copies of Building Permits of $5,000 valuation and up ($20,000 min for Re-Roofs), ($50,000 min. for Cell Tower upgrades), (Electrical, Mechanical & Plumbing at $100,000 min.) and (Solar Panels, Swimming Pools & Foundations at any valuation)  Arlington    2018-06\n",
       "3      3                                                                                                                                                                                    police report filed to an officer against Wayne Parris (DOB 08-03-1957) from Brittany J. Parris. The paperwork I have has a case number D18-39 it is also stamped at the bottom with 18-1294, Iím not sure which number you will need. If there is any other information needed please let me know.  Arlington    2018-06\n",
       "4      4                                                                                                                                                           Email Communications between Stephanie Shook, Dave Kraski, Bruce Stedman and Chad Schmidt in regards to Fire Protection District 21 billing and passage of contract for ALS Services. \\n\\nAlso any copies of Agenda Bills, D21 Contract and materials presented for review in Nov/Dec time frame in regards to the contract.  Arlington    2018-06"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert nltk part of speech tags to wordnet tags (we use this to stem the words in data cleaning below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to turn separate files of the 1000 most popular baby names by year provided by the [Social Security Administration](https://www.ssa.gov/OACT/babynames/) into a single set of unique first names across years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_list(st, end, thresh):\n",
    "    names = set()\n",
    "    for yr in range(st, end+1):\n",
    "        fp = '../data/names/yob{}.txt'.format(yr)\n",
    "        df = pd.read_table(fp, sep = ',', names = ['name', 'sex', 'count'])\n",
    "        df = df[df['count'] >= thresh]\n",
    "        #df['name'] = df['name'].str.lower()\n",
    "        names |= set(df['name'])\n",
    "    \n",
    "    return list(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fef77e480974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create list of unique first names that were on the 1000 most popular names each year between 1950-2017\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1950\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'name_list' is not defined"
     ]
    }
   ],
   "source": [
    "# create list of unique first names that were on the 1000 most popular names each year between 1950-2017\n",
    "\n",
    "names = name_list(1950,2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create list on common surnames in the United States. Data on surnames is from the U.S. Census Bureau, compiled by FiveThirtyEight and accessed via [data.world](https://data.world/fivethirtyeight/most-common-name/workspace/file?filename=README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_names = pd.read_csv('../data/names/surnames.csv')\n",
    "last_names.name = last_names.name.str.title()\n",
    "ln = list(last_names['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine first names and surnames and create dictionary \n",
    "\n",
    "all_names = names + ln\n",
    "all_names_dict = {key: 1 for key in all_names if key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean PRR data to prepare for LDA analysis\n",
    "\n",
    "Prior to analysis, we clean our unstructured text data to improve the outcome of our LDA analysis results. Our goals are as follows:\n",
    "\n",
    "1. Remove \"noise\" - words that do not provide information on the subject of a PRR (eg. stop words like \"the\", proper nouns like people's names or city names, punctuation and digits, and general words/phrase common to PRRs like \"good morning\" or \"record\"\n",
    "2. Stem words so like words are treated as the same (eg. \"photo\" and \"photos\" should be regarded as the same word, as should \"assault\" and \"assaulted\"\n",
    "3. Account for meaningful phrases where the combination of words has particular meaning (to avoid excessive computation time, we only consider two-word phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove common public record request phrases - we remove as phrases because we care about specific combination/order \n",
    "# of words (we want to remove \"open record request\" not all instances of word \"open\")\n",
    "phrase_list = ['public record request', 'open record request', 'public records request', 'open records request', \n",
    "               'foia request', 'see attached', 'see attachment', 'to whom it may concern', 'public records act',\n",
    "              'electronic copy', 'electronic copies', 'freedom of information act', 'good afternoon', 'good morning',\n",
    "              'good day']\n",
    "                         \n",
    "for phrase in phrase_list:\n",
    "    s = re.compile(re.escape(phrase), re.IGNORECASE)\n",
    "    data.Summary = data['Summary'].apply(lambda x: s.sub('', x))\n",
    "    \n",
    "    \n",
    "# Replace common acronyms in Summary\n",
    "data.Summary = data.Summary.str.replace('NOPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('OPD' , 'police department')\n",
    "data.Summary = data.Summary.str.replace('SFPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('CPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('APD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('GPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('KPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('TPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('DPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('EPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('HPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('LPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('MDPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('PPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('SPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('VPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('CCPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('FCPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('TPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('LCPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('OKCPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('PAPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('RCPD', 'police department')\n",
    "data.Summary = data.Summary.str.replace('WSPD', 'police department')\n",
    "\n",
    "# PDRD = portable digital recording device (body cam) worn by police\n",
    "data.Summary = data.Summary.str.replace('PDRD', 'police body camera')\n",
    "data.Summary = data.Summary.str.replace('CPS', 'child protective services')\n",
    "\n",
    "#https://www.sfdph.org/dph/EH/HMUPA/HMUPAFormsMenu.asp - hazardous materials\n",
    "#https://www.waterboards.ca.gov/ust/contacts/docs/lop_guide.pdf - water resources local oversight program\n",
    "data.Summary = data.Summary.str.replace('LOP', 'water')\n",
    "data.Summary = data.Summary.str.replace('HMUPA', 'hazardous materials')\n",
    "\n",
    "# Replace key numbers with strings\n",
    "data.Summary = data.Summary.str.replace(' 911 ', ' nineoneone ')\n",
    "data.Summary = data.Summary.str.replace(' 311 ', ' threeoneone ')\n",
    "data.Summary = data.Summary.str.replace(' 9-11 ', ' nineoneone ')\n",
    "data.Summary = data.Summary.str.replace(' 3-11 ', ' threeoneone ')\n",
    "\n",
    "# Remove digits\n",
    "dig_translator = str.maketrans('','', string.digits)\n",
    "data.Summary = data.Summary.str.translate(dig_translator)\n",
    "\n",
    "# because \"will\" is in the NLTK list of stopwords below, we treat 'final will' separately                         \n",
    "c = re.compile(re.escape('final will'), re.IGNORECASE)\n",
    "data.Summary = data['Summary'].apply(lambda x: s.sub('final final_will', x))\n",
    "\n",
    "# replace hyphen and slash with space to treat hyphate words as two separate words\n",
    "hyphen_translator = str.maketrans('-/','  ')\n",
    "data.Summary = data.Summary.str.translate(hyphen_translator)\n",
    "\n",
    "# remove all punctuation\n",
    "translator = str.maketrans('','', string.punctuation)\n",
    "data.Summary = data.Summary.str.translate(translator)\n",
    "\n",
    "## Use SpaCy tokenizer to ID Proper Nouns ##\n",
    "\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "#data['token_sp'] = data['Summary'].apply(lambda x: nlp(x))\n",
    "#data['pn'] = data['token_sp'].apply(lambda x: [ i.lemma_ for i in x if i.tag_ == 'NNP'])\n",
    "\n",
    "# split text into list of words by space \n",
    "data['token'] = data['Summary'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# remove proper first and last names in our dictionary + convert all words to lower case\n",
    "data['token'] = data['token'].apply(lambda x: [i.lower() for i in x if i not in all_names_dict])\n",
    "\n",
    "#remove empty strings, stopwords and stem\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lmtzr = WordNetLemmatizer()\n",
    "data['lemma'] = data['token'].apply(lambda x: nltk.pos_tag(x))\n",
    "data['mash'] = data['lemma'].apply(lambda x: [lmtzr.lemmatize(i[0], get_wordnet_pos(i[1])) for i in x if len(i[0]) > 0 and i[0] not in stop_words])\n",
    "\n",
    "# Remove whitespace\n",
    "wsp_translator = str.maketrans('','', string.whitespace)\n",
    "data['mash'] = data['mash'].apply(lambda x: [i.translate(wsp_translator) for i in x])\n",
    "\n",
    "# Remove empty lists\n",
    "data['mash_len'] = data['mash'].apply(lambda x: len(x))\n",
    "data = data[data['mash_len'] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove number suffixes\n",
    "suffix_list = ['th', 'nd', 'st', 'rd', 'blvd', 'pkwy']\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in suffix_list])\n",
    "\n",
    "# remove city and state abbreviations\n",
    "abbv_list = ['wa', 'nc', 'co', 'ca', 'oh', 'tx', 'nm', 'fl', 'ma', 'la', 'ok', 'az', 'ri', 'va', \n",
    "             'francisco', 'sf', 'okc', 'lv', 'nola', 'slc', 'cw']\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in abbv_list])\n",
    "\n",
    "# remove spelled numbers\n",
    "num_list = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten']\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in  num_list])\n",
    "\n",
    "# replace 'inc' with 'incident\n",
    "data['mash'] = data['mash'].apply(lambda x: ['incident' if i=='inc' else i for i in x])\n",
    "\n",
    "# replace 'pd' with 'police department\n",
    "data['mash'] = data['mash'].apply(lambda x: ['police department' if i=='pd' else i for i in x])\n",
    "\n",
    "# remove noise words\n",
    "noise = ['dr', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sept', 'sep', 'oct', 'nov', 'dec', \n",
    "        'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', \n",
    "        'december', 'ne', 'nw', 'se', 'sw', 'ct', 'dr', 'way', 'dv', 'ave', 'aka', 'get', 'look', 'im', 'want', \n",
    "        'find', 'could', 'go', 'take', 'e', 'n', 's', 'w', '“', '’', '”', '•', 'northeast', 'northwest', 'southeast', \n",
    "        'southwest', 'north', 'south', 'east', 'west', 'orleans', '–', 'a', 'b', 'c', 'd', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "        'l', 'm', 'o', 'p', 'q', 'r', 't', 'u', 'v', 'x', 'y', 'z', 'am', 'pm', 'hr', 'mr', 'ms', 'mrs', 'johnson', \n",
    "        'jr', 'kent', 'christopher', 'miller', 'joe', 'willows', 'david', 'michael', 'john', 'red', 'robert',\n",
    "        'ask', 'able', 'let', 'question', 'also', 'snohomish', '¬ß', 'per', 'available', 'test', '√Ø', 'andor', '·', 'etc',\n",
    "        'ï', 'ce', 'eg', 'sammamish']\n",
    "\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in noise])\n",
    "\n",
    "# remove cities\n",
    "city_list = ['arlington', 'asheville', 'bainbridge', 'island', 'boulder', 'cathedral' ,'clearwater', 'dayton', \n",
    "            'denton', 'everett', 'fort', 'collins', 'greensboro', 'hayward', 'kirkland', 'las', 'cruces', 'lynnwood',\n",
    "            'mercer', 'miami', 'middleborough', 'new', 'orleans', 'oakland', 'oklahoma', 'olympia', 'palo', 'alto', \n",
    "            'peoria', 'pullman', 'rancho', 'cucamonga', 'redmond', 'renton', 'sacramento', 'san', 'francisco', \n",
    "            'tukwila', 'vallejo', 'west', 'sacramento', 'winchester']\n",
    "\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in city_list])\n",
    "\n",
    "# Remove all state names\n",
    "state_list = ['washington', 'carolina', 'colorado', 'california',\n",
    "             'ohio', 'texas', 'florida', 'new', 'mexico','massachusetts',\n",
    "             'louisiana', 'oklahoma', 'arizona', 'rhode', 'virginia']\n",
    "\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in state_list])\n",
    "\n",
    "# Create two-word phrases (bigrams)\n",
    "data['bigrams'] = data['mash'].apply(lambda x: [\"_\".join(w) for w in ngrams(x, 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify and remove noise words that are commonly used in PRRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = [y for x in list(data['mash']) for y in x]\n",
    "counts = Counter(word_list)\n",
    "Counter(word_list).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_list = ['report', 'request', 'record', 'city', 'please', 'copy', 'date', 'information', 'would', 'regard', 'public',\n",
    "              'include', 'document', 'provide', 'like', 'thank', 'need', 'know', 'thanks', 'pursuant', 'dear', 'file',\n",
    "              'relate', 'from', 'either', 'hello', 'hi', 'foia', 'requestors', 'requestor', 'receive', 'available', \n",
    "               'make', 'attach', 'pertain', 'might', 'see', 'near']\n",
    "\n",
    "# remove general words that are common to public record requests\n",
    "data['mash'] = data['mash'].apply(lambda x: [i for i in x if i not in common_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify meaningful phrases by looking at the list of two-word sequences (bigrams) that are frequently used in public record requests. The meaningful phrases that we identify will be added to the list of words to consider in analysis for the PRRs in which they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_list = [y for x in list(data['bigrams']) for y in x]\n",
    "counts = Counter(bigram_list)\n",
    "Counter(bigram_list).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0372d7a7d2c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                  'floor_plan', 'site_plan', 'building_plan', 'building_code', 'code_enforcement', 'personnel_file']\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'common_bigrams'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bigrams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_bigrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "common_bigrams = ['police_report', 'insurance_company', 'location_loss', 'date_occurrence', 'reportcase_number',\n",
    "                  'insure_driver', 'auto_accident', 'occurrence_location', 'transactionreference_insurance', 'number_date', 'type_auto',\n",
    "                  'accident_reportcase', 'code_violation', 'copy_police', 'incident_report', 'police_department', 'certificate_occupancy',\n",
    "                  'accident_report', 'property_locate', 'storage_tank','driver_note', 'building_permit', 'driver_driver','case_number', \n",
    "                  'hazardous_material', 'collision_report', 'state_farm', 'site_plan', 'fire_department', 'ftp_report', 'auto_theft',\n",
    "                  'fire_code', 'request_police', 'farm_claim', 'claim_compass', 'site_assessment', 'compass_report', 'environmental_site', \n",
    "                  'tax_sale', 'loss_cross','city_council', 'code_enforcement', 'subject_property', 'report_case', 'phase_environmental', \n",
    "                  'report_incident', 'date_loss', 'police_case', 'witness_statement', 'driving_record', 'break_in', 'birth_certificate', \n",
    "                  'death_certificate', 'background_check', 'public_works', 'lease_agreement', 'medical_record', 'billing_record', \n",
    "                  'record_check', 'records_check', 'marriage_certificate', 'marriage_record', 'park_ticket', 'miss_person',\n",
    "                 'marriage_license', 'reckless_driving', 'arrest_report', 'medical_billing', 'medical_report', 'criminal_record',\n",
    "                 'floor_plan', 'site_plan', 'building_plan', 'building_code', 'code_enforcement', 'personnel_file']\n",
    "\n",
    "data['common_bigrams'] = data['bigrams'].apply(lambda x: [i for i in x if i in common_bigrams])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine columns containing cleaned words (mash) and meaningful phrases (common_bigrams) to yield final set of words for analysis for each PRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['final_mash'] = data['mash'] + data['common_bigrams']\n",
    "\n",
    "# Remove empty lists\n",
    "data['mash_len'] = data['final_mash'].apply(lambda x: len(x))\n",
    "data = data[data['mash_len'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see the result of the final cleaned data below. \n",
    "The final_mash column represents the set of words that will be considered in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mash_len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there is significant variation in the average request length per city. \n",
    "In some cases, cities with short average length represent cities where the provided data represented a summary of the original request (Oklahoma City) though in other cases, like Dayton, we received the raw data and the average length is still considerably shorter than other cities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gp = data.groupby('city').mean()\n",
    "data_gp['mash_len']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see a couple of examples of the cleaned mash and the original request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Summary'][data.index == 164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final_mash'][data.index == 164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final_mash'][data.index == 60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Summary\"][60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cleaned data to csv to use for testing different LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tested a number of different parameters for the LDA models to identify the optimal model. Because these models are very computationally intensive and take a long time to run, we have included the tests and final model in a [separate notebook](https://github.com/sunlightpolicy/Sunlight_FOIA/blob/master/src/analysis/LDA_Model_Tests.ipynb). Below, we conduct the analysis on our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test different length restrictions\n",
    "'''\n",
    "# 1) as-is\n",
    "\n",
    "# create dictionary and corpus\n",
    "texts = list(data['final_mash'])\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus_all = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_60_60 = gensim.models.ldamodel.LdaModel(corpus_all, num_topics=60, id2word = dictionary, \n",
    "                                         passes = 60, random_state=7)\n",
    "model_name = \"lda_60_60_model_all\"\n",
    "lda_60_60.save(model_name)\n",
    "corpus_lda = lda_60_60[corpus_all]\n",
    "corpus_lda_list = list(corpus_lda)\n",
    "topics = data.copy()\n",
    "topics = topics.assign(topic_comp = corpus_lda_list)\n",
    "file_name = \"topics/lda_60_60_topics_all.csv\"\n",
    "topics.to_csv(file_name)\n",
    "'''\n",
    "\n",
    "# 2) mash len > 2\n",
    "\n",
    "data_ml2 = data[data['mash_len'] > 2]\n",
    "texts = list(data_ml2['final_mash'])\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus_ml2 = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_100_60_ml2 = gensim.models.ldamodel.LdaModel(corpus_ml2, num_topics=100, id2word = dictionary, \n",
    "                                         passes = 60, random_state=7)\n",
    "model_name = \"lda_100_60_model_ml2\"\n",
    "lda_100_60_ml2.save(model_name)\n",
    "corpus_lda = lda_100_60_ml2[corpus_ml2]\n",
    "corpus_lda_list = list(corpus_lda)\n",
    "topics = data_ml2.copy()\n",
    "topics = topics.assign(topic_comp = corpus_lda_list)\n",
    "file_name = \"topics/lda_100_60_topics_ml2.csv\"\n",
    "topics.to_csv(file_name)\n",
    "\n",
    "# 3) mash len > 3\n",
    "\n",
    "data_ml3 = data[data['mash_len'] > 3]\n",
    "texts = list(data_ml3['final_mash'])\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus_ml3 = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_100_60_ml3 = gensim.models.ldamodel.LdaModel(corpus_ml3, num_topics=100, id2word = dictionary, \n",
    "                                         passes = 60, random_state=7)\n",
    "model_name = \"lda_100_60_model_ml3\"\n",
    "lda_100_60_ml3.save(model_name)\n",
    "corpus_lda = lda_100_60_ml3[corpus_ml3]\n",
    "corpus_lda_list = list(corpus_lda)\n",
    "topics = data_ml3.copy()\n",
    "topics = topics.assign(topic_comp = corpus_lda_list)\n",
    "file_name = \"topics/lda_100_60_topics_ml3.csv\"\n",
    "topics.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ml = data[data['mash_len'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dictionary and corpus\n",
    "texts = list(data['final_mash'])\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 30 topics and 30 passes\n",
    "lda_30_45 = gensim.models.ldamodel.LdaModel(corpus, num_topics=30, id2word = dictionary, \n",
    "                                         passes = 30, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show topics for model\n",
    "lda_30_45.show_topics(num_topics=30, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show topics for model\n",
    "lda_50_45_2.show_topics(num_topics=50, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save good model\n",
    "lda_40_45.save('lda_40_45_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_40_45 = gensim.models.ldamodel.LdaModel.load('lda_40_45_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_lda = lda_40_45[corpus] #this is just a wrapper; calculates on the fly when you call it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_lda_list = list(corpus_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_30_45[corpus[600]], key=lambda tup: -1*tup[1]): #600th document\n",
    "    print(\"Score: {}\\t Topic: {} \\n\".format(score, lda_30_45.print_topic(index, 15))) #15 word topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mash2[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = data.copy()\n",
    "topics = topics.assign(topic_comp = corpus_lda_list)\n",
    "topics.head() #the topic_comp column are actual Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highest_topic(fp):\n",
    "    df = pd.read_csv(fp)\n",
    "    df['topic_comp'] =  df['topic_comp'].apply(lambda x:  ast.literal_eval(x))\n",
    "    df['comp_len'] = df['topic_comp'].apply(len)\n",
    "    df = df[df['comp_len'] > 0]\n",
    "    df['top_topic'] = df['topic_comp'].apply(lambda x: max(x, key=lambda item:item[1])[0])\n",
    "    df['top_topic_comp'] = df['topic_comp'].apply(lambda x: max(x, key=lambda item:item[1])[1])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topics_to_csv(df, num_topics):\n",
    "    for topic in range(0, num_topics):\n",
    "        subset = df[df['top_topic'] == topic]\n",
    "        file_name = 'topics/{}_PRR_topic_{}.csv'.format(num_topics, topic)\n",
    "        subset.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_csv(model_list):\n",
    "    for model in model_list:\n",
    "        fp = 'topics/lda_{}_45_topics.csv'.format(model)\n",
    "        df = pd.read_csv(fp)\n",
    "        highest_topic(df, fp)\n",
    "        topics_to_csv(df, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp = 'topics/lda_data_c2000_3.csv'\n",
    "highest_topic(fp)\n",
    "topics_to_csv(fp, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp2 = 'topics/lda_data10.csv'\n",
    "highest_topic(fp2)\n",
    "topics_to_csv(fp2, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = highest_topic('topics/lda_data_avg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['comp_len'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_blank = df[df['comp_len'] == 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We test a variety of different numbers of topics to identify the number of topics that yields the best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics_list = [20, 30, 40, 50, 60]\n",
    "for n in num_topics_list:\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus, num_topics=n, id2word = dictionary, \n",
    "                                         passes = 60, random_state=7)\n",
    "    \n",
    "    model_name = \"lda_{}_45_model\".format(n)\n",
    "    lda.save(model_name)\n",
    "    corpus_lda = lda[corpus]\n",
    "    corpus_lda_list = list(corpus_lda)\n",
    "    topics = data.copy()\n",
    "    topics = topics.assign(topic_comp = corpus_lda_list)\n",
    "    file_name = \"topics/lda_{}_45_topics.csv\".format(n)\n",
    "    topics.to_csv(file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('topics/lda_60_45_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('top_topic').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('city').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_csv(num_topics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topic_comp'][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create small dataset for testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sm = data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "highest_topic(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = data[~data['top_topic'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['top_topic'].hist(bins=50)\n",
    "plt.title('Count by Topic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_50_45 = gensim.models.ldamodel.LdaModel.load('lda_50_45_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_to_csv(data, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_cat(df, num_topics, city):\n",
    "    df_raw = pd.read_csv('/Users/alenastern/Google Drive File Stream/My Drive/Alena_Project/PR_Data/{}.csv'.format(city))\n",
    "    \n",
    "    df_raw_gp = df_raw.groupby('Dept').count()\n",
    "    df_raw_gp['Summary'].plot(kind = 'bar')\n",
    "    title = 'PRR Departments for {}'.format(city)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    df_city = df[df['city'] == city]\n",
    "    df_merge = df_city.merge(df_raw, how ='left', on = 'Summary')\n",
    "    df_merge['top_topic'] = pd.to_numeric(df_merge['top_topic'])\n",
    "    for topic in range(0, num_topics):\n",
    "        df_sub = df_merge[df_merge['top_topic'] == topic]\n",
    "        df_gp = df_sub.groupby('Dept').count()\n",
    "        df_gp['Summary'].plot(kind='bar')\n",
    "        title = 'Departments for topic {} in {}'.format(topic, city)\n",
    "        plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "city_list = ['Bainbridge', 'LasCruces', 'FortCollins', 'Mercer', 'Nola', 'Oakland', 'PaloAlto', 'Redmond', \n",
    "             'SanFrancisco', 'Vallejo']\n",
    "fp = 'topics/lda_20_45_topics.csv'\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cat(df, 20, 'PaloAlto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = gensim.models.ldamodel.LdaModel.load('lda_data_avg_2')\n",
    "final_fp = 'topics/lda_data_avg_2.csv'\n",
    "final_model.show_topics(num_topics=60, formatted=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Best Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model = gensim.models.ldamodel.LdaModel.load('lda_data_c2000_3_2')\n",
    "final_fp = 'topics/lda_data_c2000_3_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.show_topics(num_topics=60, formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Most Popular Catgories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Winner take all\" popularity metric:\n",
    "* Scoring Rules:\n",
    "    * Only the topic that composes the largest share of a document scores \"points\" for its \"Adjusted Popularity\" total.\n",
    "    * If a topic composes the largest share of that document, its \"points\" are its composition score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize data by final model identify highest topics\n",
    "final_df = highest_topic(final_fp)\n",
    "final_df = final_df[['top_topic', 'top_topic_comp']]\n",
    "\n",
    "topic_gp = final_df.groupby('top_topic').sum()\n",
    "topic_gp.reset_index(inplace = True)\n",
    "topic_gp.rename(index=str, columns={\"top_topic_comp\": \"total_pop\"}, inplace = True)\n",
    "topic_gp = topic_gp[['top_topic', 'total_pop']]\n",
    "\n",
    "topic_gp.sort_values(by=['top_topic'], ascending = True)\n",
    "\n",
    "# add in topic words\n",
    "words_in_topics = [tup[1] for tup in final_model.show_topics(num_topics=60, formatted=False)] # update w/ winning model\n",
    "topic_gp['topic'] = words_in_topics\n",
    "\n",
    "topic_gp[[\"topic1\", \"topic2\", \"topic3\", \"topic4\", \n",
    "       \"topic5\", \"topic6\", \"topic7\", \"topic8\", \"topic9\", \"topic10\"]] = topic_gp.topic.apply(pd.Series)\n",
    "\n",
    "topic_gp = topic_gp.sort_values(by='total_pop', ascending=False)\n",
    "topic_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Winner Take All with Thresholds\" Rules:\n",
    "\n",
    "Scoring Rules:\n",
    "* Same as \"Winner Take All\", except a winning topic must compose at least a certain threshold of a document to get any points.\n",
    "* We'll try 0.2 (low) and 0.5 (high) thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def winner_thresh(fp, thresh):\n",
    "    final_df = highest_topic(fp)\n",
    "    final_df = final_df[['top_topic', 'top_topic_comp']]\n",
    "\n",
    "\n",
    "    final_df = final_df[final_df['top_topic_comp'] >= thresh]\n",
    "    topic_gp = final_df.groupby('top_topic').sum()\n",
    "    topic_gp.reset_index(inplace = True)\n",
    "    topic_gp.rename(index=str, columns={\"top_topic_comp\": \"total_pop\"}, inplace = True)\n",
    "    topic_gp = topic_gp[['top_topic', 'total_pop']]\n",
    "\n",
    "    topic_gp.sort_values(by=['top_topic'], ascending = True)\n",
    "\n",
    "    # add in topic words\n",
    "    words_in_topics = [tup[1] for tup in final_model.show_topics(num_topics=60, formatted=False)] # update w/ winning model\n",
    "    topic_gp['topic'] = words_in_topics\n",
    "\n",
    "    topic_gp[[\"topic1\", \"topic2\", \"topic3\", \"topic4\", \n",
    "       \"topic5\", \"topic6\", \"topic7\", \"topic8\", \"topic9\", \"topic10\"]] = topic_gp.topic.apply(pd.Series)\n",
    "\n",
    "    topic_gp = topic_gp.sort_values(by='total_pop', ascending=False)\n",
    "    return topic_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic2 = winner_thresh(final_fp, 0.2)\n",
    "topic5 = winner_thresh(final_fp, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Credit Approach\n",
    "\n",
    "* All topics assigned to a given PRR get credit for that PRR's topic composition score, provided the score is above the established threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prop_calc(fp, thresh):\n",
    "    df = pd.read_csv(fp)\n",
    "    results_dict = {}\n",
    "    df['topic_comp'] =  df['topic_comp'].apply(lambda x:  ast.literal_eval(x))\n",
    "    for row_num in df.index:\n",
    "        for tup in df.topic_comp[row_num]: \n",
    "            if not tup[0] in results_dict:\n",
    "                if tup[1] >= thresh: \n",
    "                    results_dict[tup[0]] = tup[1] \n",
    "                else:\n",
    "                    pass\n",
    "            if tup[0] in results_dict:\n",
    "                if tup[1] >= thresh:\n",
    "                    results_dict[tup[0]] += tup[1] \n",
    "                    \n",
    "    pd_df = pd.DataFrame.from_dict(results_dict, orient = 'index')\n",
    "    pd_df.reset_index(inplace = True)\n",
    "    pd_df.rename(index = str, columns = {'index': 'topic', 0: 'total_score'}, inplace = True)\n",
    "    pd_df.topic = pd.to_numeric(pd_df.topic)\n",
    "    pd_df = pd_df.sort_values(by=['topic'], ascending = True)\n",
    "\n",
    "    # add in topic words\n",
    "    words_in_topics = [tup[1] for tup in final_model.show_topics(num_topics=60, formatted=False)] # update w/ winning model\n",
    "    pd_df['topic_words'] = words_in_topics\n",
    "\n",
    "    pd_df[[\"topic1\", \"topic2\", \"topic3\", \"topic4\", \n",
    "           \"topic5\", \"topic6\", \"topic7\", \"topic8\", \"topic9\", \"topic10\"]] = pd_df.topic_words.apply(pd.Series)\n",
    "\n",
    "    \n",
    "    \n",
    "    pd_df = pd_df.sort_values(by='total_score', ascending=False)\n",
    "\n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc2 = prop_calc(final_fp, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_10 = list(pc2.topic[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_calc(final_fp, .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Metrics within a City/County (Dampened Popularity):\n",
    "* For each city/county, we add up total score fore each topic and then take the log of the total score. We then add up scores across each city/count.\n",
    "* For winner-take-all, only score for top topic included (provided it is above threshold)\n",
    "* For partial-credit, scores for all topics included (provided it is above threhsold\n",
    "* This is an extra control for cities with a large number of PRRs from skewing our results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_list = ['Arlington', 'Asheville', 'Bainbridge', 'Boulder', 'CathedralCity' ,'Clearwater', 'Dayton', \n",
    "            'Denton', 'Everett', 'FortCollins', 'Greensboro', 'Hayward', 'Kirkland', 'LasCruces', 'Lynnwood',\n",
    "            'Mercer', 'Miami', 'Middleborough', 'Nola', 'Oakland', 'OKC', 'Olympia', 'PaloAlto', \n",
    "            'Peoria', 'Pullman', 'RanchoCucamonga', 'Redmond', 'Renton', 'Sacramento', 'SanFrancisco', \n",
    "            'Tukwila', 'Vallejo', 'WestSacramento', 'Winchester']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_pop(fp, city_list, thresh, winner_take_all):\n",
    "    df = pd.read_csv(fp)\n",
    "    df['topic_comp'] =  df['topic_comp'].apply(lambda x:  ast.literal_eval(x))\n",
    "    \n",
    "    list_of_domain_dicts = []\n",
    "    popularity_dict = {}\n",
    "    \n",
    "    for city in city_list:\n",
    "        \n",
    "        results_dict = {}\n",
    "        \n",
    "        #get our df only of rows from a given city/state domain\n",
    "        city_df = df[df.city == city]\n",
    "        \n",
    "        for row_num in city_df.index:\n",
    "            tup_list = city_df.topic_comp[row_num] #list of (topic, doc composition) tuples\n",
    "            \n",
    "            if winner_take_all:\n",
    "        \n",
    "                #return only the tuple w/highest topic composition value\n",
    "                winner_tuple = max(tup_list, key=lambda item:item[1])  \n",
    "\n",
    "                if not winner_tuple[0] in results_dict: #if not in dict, add it with its TOTAL VIEWS score\n",
    "                    if winner_tuple[1] > thresh:\n",
    "                        results_dict[winner_tuple[0]] = winner_tuple[1] \n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                if winner_tuple[0] in results_dict: #if in dict, increment that key's value with score\n",
    "                    if winner_tuple[1] > thresh:\n",
    "                        results_dict[winner_tuple[0]] += winner_tuple[1]\n",
    "                    pass\n",
    "            else:\n",
    "                for tup in tup_list: \n",
    "                    if not tup[0] in results_dict:\n",
    "                        if tup[1] >= thresh: \n",
    "                            results_dict[tup[0]] = tup[1] \n",
    "                    else:\n",
    "                        pass\n",
    "                    if tup[0] in results_dict:\n",
    "                        if tup[1] >= thresh:\n",
    "                            results_dict[tup[0]] += tup[1] \n",
    "            \n",
    "        #when loop of domain_df is finished, take log of all keys in dict\n",
    "        log_dict = {}\n",
    "        for k,v in results_dict.items():\n",
    "            log_dict[k] = np.log(v)\n",
    "        \n",
    "        #now we have a polished dict of topic numbers as keys and log of all views/DLs as values; append it to list\n",
    "        list_of_domain_dicts.append(log_dict)\n",
    "    \n",
    "    #use Counter() object to sync our dictionaries\n",
    "    c = Counter()\n",
    "    for d in list_of_domain_dicts:\n",
    "        c.update(d)\n",
    "    \n",
    "    popularity_dict = dict(c)\n",
    "    \n",
    "    pd_df = pd.DataFrame.from_dict(popularity_dict, orient = 'index')\n",
    "    pd_df.reset_index(inplace = True)\n",
    "    pd_df.rename(index = str, columns = {'index': 'topic', 0: 'total_score'}, inplace = True)\n",
    "    pd_df.topic = pd.to_numeric(pd_df.topic)\n",
    "    pd_df = pd_df.sort_values(by=['topic'], ascending = True)\n",
    "\n",
    "    # add in topic words\n",
    "    words_in_topics = [tup[1] for tup in final_model.show_topics(num_topics=60, formatted=False)] # update w/ winning model\n",
    "    pd_df['topic_words'] = words_in_topics\n",
    "\n",
    "    pd_df[[\"topic1\", \"topic2\", \"topic3\", \"topic4\", \n",
    "           \"topic5\", \"topic6\", \"topic7\", \"topic8\", \"topic9\", \"topic10\"]] = pd_df.topic_words.apply(pd.Series)\n",
    "\n",
    "    \n",
    "    \n",
    "    pd_df = pd_df.sort_values(by='total_score', ascending=False)\n",
    "\n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pop(final_fp, city_list, .5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pop(final_fp, city_list, .2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pop('topics/lda_data_avg.csv', city_list, .5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pop('topics/lda_data_avg.csv', city_list, .2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Topic Popularity by City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pop_by_city(fp, city_list, num_topics, thresh, winner_take_all):\n",
    "    \n",
    "    df = pd.read_csv(fp)\n",
    "    df['topic_comp'] =  df['topic_comp'].apply(lambda x:  ast.literal_eval(x))\n",
    "    \n",
    "    cols = ['city', 'topic', 'total_pop', 'pct'] \n",
    "    \n",
    "    topic_pop_city = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for city in city_list:\n",
    "        \n",
    "        results_dict = {}\n",
    "        for i in range(0, num_topics):\n",
    "            results_dict[i] = 0\n",
    "        \n",
    "        #get our df only of rows from a given city/state domain\n",
    "        city_df = df[df.city == city]\n",
    "        \n",
    "        for row_num in city_df.index:\n",
    "            tup_list = city_df.topic_comp[row_num] #list of (topic, doc composition) tuples\n",
    "            \n",
    "            if winner_take_all:\n",
    "        \n",
    "                #return only the tuple w/highest topic composition value\n",
    "                winner_tuple = max(tup_list, key=lambda item:item[1])  \n",
    "\n",
    "                if not winner_tuple[0] in results_dict: #if not in dict, add it with its TOTAL VIEWS score\n",
    "                    if winner_tuple[1] > thresh:\n",
    "                        results_dict[winner_tuple[0]] = winner_tuple[1] \n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                if winner_tuple[0] in results_dict: #if in dict, increment that key's value with score\n",
    "                    if winner_tuple[1] > thresh:\n",
    "                        results_dict[winner_tuple[0]] += winner_tuple[1]\n",
    "                    pass\n",
    "            else:\n",
    "                for tup in tup_list: \n",
    "                    if not tup[0] in results_dict:\n",
    "                        if tup[1] >= thresh: \n",
    "                            results_dict[tup[0]] = tup[1] \n",
    "                    else:\n",
    "                        pass\n",
    "                    if tup[0] in results_dict:\n",
    "                        if tup[1] >= thresh:\n",
    "                            results_dict[tup[0]] += tup[1] \n",
    "        \n",
    "        pd_df = pd.DataFrame.from_dict(results_dict, orient = 'index')\n",
    "        pd_df.reset_index(inplace = True)\n",
    "        pd_df.rename(index = str, columns = {'index': 'topic', 0: 'total_pop'}, inplace = True)\n",
    "        pd_df['city'] = city\n",
    "        pd_df['pct'] = (pd_df['total_pop']/sum(pd_df['total_pop']))*100\n",
    "        topic_pop_city = pd.concat([topic_pop_city, pd_df])\n",
    "\n",
    "    return topic_pop_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbc = pop_by_city(final_fp, city_list, 60, .2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbc.to_csv('pbc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Popularity Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic popularity(city):\n",
    "    avg_month.plot(x='date_posted', y = ['total_price_excluding_optional_support', 'total_price_including_optional_support'], kind = 'line' )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sp = pd.read_csv('topics/lda_data_sp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sp['final_mash'] = df_sp['final_mash'].apply(lambda x:  ast.literal_eval(x))\n",
    "df_sp['mash'] = df_sp['mash'].apply(lambda x:  ast.literal_eval(x))\n",
    "df_sp['common_bigrams'] = df_sp['common_bigrams'].apply(lambda x:  ast.literal_eval(x))\n",
    "df_sp['token'] = df_sp['token'].apply(lambda x:  ast.literal_eval(x))\n",
    "df_sp['lemma'] = df_sp['lemma'].apply(lambda x:  ast.literal_eval(x))\n",
    "df_sp['pn2'] = df_sp['pn2'].apply(lambda x:  ast.literal_eval(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pn_list = [y for x in list(df_sp['pn2']) for y in x]\n",
    "pn_counts = Counter(pn_list)\n",
    "words = list(pn_counts.keys())\n",
    "cnt = list(pn_counts.values())\n",
    "pn_count_df = pd.DataFrame({'word': words, 'cnt': cnt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_count_df.cnt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pn_count_df4 = pn_count_df[pn_count_df['cnt'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_count_df4[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pn_count_df.sort_values(by = ['cnt'], ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_count_df[1000:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
