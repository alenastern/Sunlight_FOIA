{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# system tools\n",
    "import warnings\n",
    "import json\n",
    "import sys\n",
    "import string\n",
    "\n",
    "# data cleaning + analysis tools\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#nltk tools\n",
    "import lda #Latent Dirichlet Allocation (create topics)\n",
    "import gensim\n",
    "from gensim import corpora, models #for constructing document term matrix\n",
    "#from stop_words import get_stop_words\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#set notebook preferences\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import JSON file with city metadata - including which cities have sufficient Public Record Request (PRR) data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = '../data/cities.json'\n",
    "\n",
    "with open(json_file, 'r') as f:\n",
    "    md = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create dataframe of PRR data for all relevant cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arlington city\n",
      "Asheville city\n",
      "Bainbridge Island city\n",
      "Boulder County\n",
      "Cathedral City city\n",
      "Dayton city\n",
      "Denton city\n",
      "Everett city\n",
      "Fort Collins city\n",
      "Greensboro city\n",
      "Hayward city\n",
      "Kirkland city\n",
      "Las Cruces city\n",
      "Lynnwood city\n",
      "Mercer Island city\n",
      "Miami city\n",
      "Middleborough town\n",
      "New Orleans city\n",
      "Oakland city\n",
      "Oklahoma City city\n",
      "Olympia city\n",
      "Palo Alto city\n",
      "Peoria city\n",
      "Pullman city\n",
      "Rancho Cucamonga city\n",
      "Redmond city\n",
      "Renton city\n",
      "Sacramento city\n",
      "San Francisco city\n",
      "Tukwila city\n",
      "Vallejo city\n",
      "West Sacramento city\n",
      "Winchester city\n"
     ]
    }
   ],
   "source": [
    "data_raw = pd.DataFrame(columns = ['city', 'month_year', 'Summary'])\n",
    "city_list = []\n",
    "for key, value in md.items():\n",
    "    city = value['name']\n",
    "    filepath = '/Users/alenastern/Google Drive File Stream/My Drive/Alena_Project/PR_Data/{}.csv'.format(city)\n",
    "    if value[\"desc\"] == \"Y\":\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding='mac_roman')\n",
    "            except:\n",
    "                continue\n",
    "        print(key)\n",
    "        name = key.split(' ')\n",
    "        city_list.append(name[0].lower())\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df['Create Date'] = pd.to_datetime(df['Create Date'])\n",
    "    except:\n",
    "        df['New'] = pd.to_datetime(df['Create Date'].apply(lambda x: re.findall('^\\S*', x)[0]))\n",
    "        df.drop(columns=['Create Date'], inplace = True)\n",
    "        df.rename(index=str, columns={\"New\": \"Create Date\"}, inplace = True)\n",
    "\n",
    "    df['month_year'] = df['Create Date'].dt.to_period('M')\n",
    "    \n",
    "    mc = df[['month_year', 'Summary']]\n",
    "    mc['city'] = city\n",
    "    \n",
    "    data_raw = pd.concat([data_raw, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see the raw data below. Our raw dataset includes 86,416 PRRs from 33 different cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Summary</th>\n",
       "      <th>city</th>\n",
       "      <th>month_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>We are working with an engineering firm on an ...</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Need copies of contracts and all related docum...</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Copies of Building Permits of $5,000 valuation...</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>police report filed to an officer against Wayn...</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Email Communications between Stephanie Shook, ...</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>2018-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            Summary       city month_year\n",
       "0      0  We are working with an engineering firm on an ...  Arlington    2018-06\n",
       "1      1  Need copies of contracts and all related docum...  Arlington    2018-06\n",
       "2      2  Copies of Building Permits of $5,000 valuation...  Arlington    2018-06\n",
       "3      3  police report filed to an officer against Wayn...  Arlington    2018-06\n",
       "4      4  Email Communications between Stephanie Shook, ...  Arlington    2018-06"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86416, 3)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_raw.city.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.index = pd.RangeIndex(len(data_raw.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_raw.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe for cleaning by removing null summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw.dropna(subset=['Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert nltk part of speech tags to wordnet tags (we use this to stem the words in data cleaning below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean PRR data to prepare for LDA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn to lowercase\n",
    "data.Summary = data.Summary.str.lower()\n",
    "\n",
    "# Remove all punctuation\n",
    "translator = str.maketrans('','', string.punctuation)\n",
    "data.Summary = data.Summary.str.translate(translator)\n",
    "\n",
    "# Remove all city names\n",
    "for city in city_list:\n",
    "    data.Summary = data.Summary.str.replace(city, '')\n",
    "    \n",
    "# Remove digits\n",
    "dig_translator = str.maketrans('','', string.digits)\n",
    "data.Summary = data.Summary.str.translate(dig_translator)\n",
    "\n",
    "#remove empty strings, stopwords and stem\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lmtzr = WordNetLemmatizer()\n",
    "data['token'] = data['Summary'].apply(lambda x: nltk.word_tokenize(x))\n",
    "data['lemma'] = data['token'].apply(lambda x: nltk.pos_tag(x))\n",
    "data['mash'] = data['lemma'].apply(lambda x: [lmtzr.lemmatize(i[0], get_wordnet_pos(i[1])) for i in x if len(i[0]) > 0 and i[0] not in stop_words])\n",
    "\n",
    "# Remove whitespace\n",
    "wsp_translator = str.maketrans('','', string.whitespace)\n",
    "data['mash'] = data['mash'].apply(lambda x: [i.translate(wsp_translator) for i in x])\n",
    "\n",
    "# Remove empty lists\n",
    "data['mash_len'] = data['mash'].apply(lambda x: len(x))\n",
    "data = data[data['mash_len'] > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and remove commonly used words in PRRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = [y for x in list(data['mash']) for y in x]\n",
    "counts = Counter(word_list)\n",
    "Counter(word_list).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(word_list).most_common()[-50:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_list = ['report', 'request', 'record', 'city', 'please', 'copy', 'date', 'information', 'would',\n",
    "              'include', 'document', 'provide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove general words that are common to public record requests\n",
    "\n",
    "for word in common_list:\n",
    "    for i in range(len(data['mash'])):\n",
    "        if word in data['mash'].loc[i]:\n",
    "            data['mash'].loc[i].remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create column with the length of mash for each PRR\n",
    "\n",
    "data['mash_len'] = data['mash'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove entries of length 0\n",
    "\n",
    "data = data[data['mash_len'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    73946.000000\n",
       "mean        20.043451\n",
       "std         36.064379\n",
       "min          1.000000\n",
       "25%          4.000000\n",
       "50%         10.000000\n",
       "75%         24.000000\n",
       "max       2924.000000\n",
       "Name: mash_len, dtype: float64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['mash_len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see a couple of examples of the cleaned mash and the original request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['build', 'permit', 'conversion', 'portion', 'garage', 'live', 'space']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['mash'].iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Building permit from 2000 for the conversion of a portion of the garage to living space.'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw[\"Summary\"].iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['build',\n",
       " 'permit',\n",
       " 'valuation',\n",
       " 'min',\n",
       " 'reroofs',\n",
       " 'min',\n",
       " 'cell',\n",
       " 'tower',\n",
       " 'upgrades',\n",
       " 'electrical',\n",
       " 'mechanical',\n",
       " 'plumbing',\n",
       " 'min',\n",
       " 'solar',\n",
       " 'panel',\n",
       " 'swim',\n",
       " 'pools',\n",
       " 'foundation',\n",
       " 'valuation',\n",
       " 'issue',\n",
       " 'may',\n",
       " 'thru',\n",
       " 'may']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['mash'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copies of Building Permits of $5,000 valuation and up ($20,000 min for Re-Roofs), ($50,000 min. for Cell Tower upgrades), (Electrical, Mechanical & Plumbing at $100,000 min.) and (Solar Panels, Swimming Pools & Foundations at any valuation)     \\nIssued  ____May 3, 2018________  thru __May 25, 2018______'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw[\"Summary\"].iloc[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dictionary and corpus\n",
    "texts = list(data['mash'])\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 50 topics and 45 passes\n",
    "lda_50_45 = gensim.models.ldamodel.LdaModel(corpus, num_topics=50, id2word = dictionary, \n",
    "                                         passes = 45, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show topics for model\n",
    "lda_50_45.show_topics(num_topics=50, formatted=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
