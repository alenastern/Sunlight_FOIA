{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "import bs4\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes the full PRR archive from all of the cities in our sample that have [Next Request](https://www.nextrequest.com/) online public record request portals. These portals publish the full archive of public record requests but do not offer an option to export the data from the website. Accordingly, we use webscraping to scrape the full archive of data from the respective portals. Note that this code will scrape the full archive starting with the most recent requests on the date it is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data from Albuquerque Next Request portal which has different url/data format than others\n",
    "\n",
    "cities = ['Albuquerque']\n",
    "\n",
    "for city in cities:\n",
    "    fn = '{}.csv'.format(city)\n",
    "    with open(fn, 'w') as csvfile:\n",
    "        fieldnames = ['Reference No', 'Request Status', 'requester', 'request', 'departments', 'cost', 'PoC', 'Create Date', 'Close Date']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames = fieldnames, delimiter = ',')\n",
    "\n",
    "        writer.writeheader()\n",
    "        base_url = 'https://nextrequest.cabq.gov/requests'\n",
    "        \n",
    "        req = requests.get(base_url)\n",
    "        html = req.text.encode('utf8')\n",
    "        soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "        \n",
    "        num_rec = int(soup.find_all('h2')[1].span.get_text().lstrip().rstrip())\n",
    "        print(num_rec)\n",
    "        num_page = math.ceil(num_rec/25)\n",
    "        print(num_page)\n",
    "\n",
    "        for i in range(2, num_page + 2):\n",
    "            rl = soup.tbody.find_all('tr', class_ = \" demo-data-false\") \n",
    "            for prr in rl:\n",
    "                cl = list(prr.children)\n",
    "                # get information from main page\n",
    "                req_id = cl[1].a.strong.contents[0]\n",
    "                status = cl[3]['class'][1]\n",
    "                requester = cl[5].get_text().lstrip().rstrip()\n",
    "                departments = cl[9].get_text().lstrip().rstrip()\n",
    "                cost = cl[11].get_text().lstrip().rstrip().strip('$')\n",
    "                poc = cl[13].get_text().lstrip().rstrip()\n",
    "                \n",
    "                #get information from request detail page\n",
    "                url_det = base_url + \"/\" + req_id\n",
    "                req_det = requests.get(url_det)\n",
    "                html = req_det.text.encode('utf8')\n",
    "                soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "                rdl = soup.find_all(id = 'request-text')\n",
    "                try:\n",
    "                    #request = rdl[0].p.contents[0].lstrip().rstrip()\n",
    "                    request = rdl[0].get_text().rstrip().lstrip()\n",
    "                except:\n",
    "                    request = 'error'\n",
    "                    print(req_id)\n",
    "                rd = soup.find_all('p', class_= \"request_date\") \n",
    "                create_date = re.findall(r'([^\\n]+)', rd[0].get_text().lstrip().rstrip())[0]\n",
    "                time= soup.find_all('span', class_ = 'time-quotes')\n",
    "                if status == 'closed' and len(time) > 0:\n",
    "                    close_date = time[0].get_text().lstrip().rstrip()\n",
    "                else:\n",
    "                    close_date = 'NaN'\n",
    "                \n",
    "                #write to csv\n",
    "                writer.writerow({'Reference No': req_id,\n",
    "                                'Request Status': status,\n",
    "                                'requester': requester,\n",
    "                                'request': request,\n",
    "                                'departments': departments,\n",
    "                                'cost': cost,\n",
    "                                'PoC': poc,\n",
    "                                'Create Date': create_date,\n",
    "                                'Close Date': close_date})\n",
    "                \n",
    "            url = 'https://nextrequest.cabq.gov/requests?requests_smart_listing[page]={}'.format(i)\n",
    "            req = requests.get(url)\n",
    "            html = req.text.encode('utf8')\n",
    "            soup = bs4.BeautifulSoup(html, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data from Next Request portals from list of cities given below\n",
    " \n",
    "cities = ['bainbridgewa', 'cityoflascruces', 'mercerisland', 'miami','middleboroughma', 'nola', 'oaklandca', \n",
    "          'providenceri', 'sanfrancisco', 'vallejo','westsacramento']\n",
    "\n",
    "for city in cities:\n",
    "    print(city)\n",
    "    fn = '{}.csv'.format(city)\n",
    "    with open(fn, 'w') as csvfile:\n",
    "        fieldnames = ['Reference No', 'Request Status', 'requester', 'request', 'departments', 'PoC', 'Create Date', 'Close Date']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames = fieldnames, delimiter = ',')\n",
    "\n",
    "        writer.writeheader()\n",
    "        url = 'https://{}.nextrequest.com/requests'.format(city)\n",
    "        print(url)\n",
    "        \n",
    "        req = requests.get(url)\n",
    "        html = req.text.encode('utf8')\n",
    "        soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "        \n",
    "        num_rec = int(soup.find_all('h2')[1].span.get_text().lstrip().rstrip())\n",
    "        print(num_rec)\n",
    "        num_page = math.ceil(num_rec/25)\n",
    "        print(num_page)\n",
    "\n",
    "        for i in range(2, num_page + 2):\n",
    "            rl = soup.tbody.find_all('tr', class_ = \" demo-data-false\") \n",
    "            for prr in rl:\n",
    "                cl = list(prr.children)\n",
    "                # get information from main page\n",
    "                req_id = cl[1].a.strong.contents[0]\n",
    "                status = cl[3]['class'][1]\n",
    "                requester = cl[5].get_text().lstrip().rstrip()\n",
    "                departments = cl[9].get_text().lstrip().rstrip()\n",
    "                poc = cl[11].get_text().lstrip().rstrip()        \n",
    "                \n",
    "            #get information from request detail page\n",
    "                url_det = url + \"/\" + req_id\n",
    "                req_det = requests.get(url_det)\n",
    "                html = req_det.text.encode('utf8')\n",
    "                soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "                rdl = soup.find_all(id = 'request-text')\n",
    "                try:\n",
    "                    rdlz = rdl[0]\n",
    "                except:\n",
    "                    print(url_det)\n",
    "                try:\n",
    "                    #request = rdl[0].p.contents[0].lstrip().rstrip()\n",
    "                    request = rdlz.get_text().rstrip().lstrip()\n",
    "                except:\n",
    "                    request = 'error'\n",
    "                    print(url_det)\n",
    "                rd = soup.find_all('p', class_= \"request_date\") \n",
    "                rdz = rd[0] \n",
    "                try:\n",
    "                    create_date = re.findall(r'([^\\n]+)', rdz.get_text().lstrip().rstrip())[0]\n",
    "                except:\n",
    "                    create_date = 'error'\n",
    "                    print(url_det)\n",
    "                time= soup.find_all('span', class_ = 'time-quotes')\n",
    "                if status == 'closed' and len(time) > 0:\n",
    "                    tz = time[0]\n",
    "                    try:\n",
    "                        close_date = tz.get_text().lstrip().rstrip()\n",
    "                    except:\n",
    "                        close_date = 'NaN'\n",
    "                        print(url_det)\n",
    "                \n",
    "                \n",
    "                #write to csv\n",
    "                writer.writerow({'Reference No': req_id,\n",
    "                                'Request Status': status,\n",
    "                                'requester': requester,\n",
    "                                'request': request,\n",
    "                                'departments': departments,\n",
    "                                'PoC': poc,\n",
    "                                'Create Date': create_date,\n",
    "                                'Close Date': close_date})\n",
    "                    \n",
    "            url_2 = url+'?requests_smart_listing[page]={}'.format(i)\n",
    "            req = requests.get(url_2)\n",
    "            html = req.text.encode('utf8')\n",
    "            soup = bs4.BeautifulSoup(html, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
