{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "import bs4\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18-3673\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://nextrequest.cabq.gov/requests'\n",
    "req = requests.get(base_url)\n",
    "\n",
    "for i in range(2, 479):\n",
    "    html = req.text.encode('utf8')\n",
    "    soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "    rl = soup.tbody.find_all('tr', class_ = \" demo-data-false\") \n",
    "    for prr in rl:\n",
    "        cl = list(prr.children)\n",
    "        # get information from main page\n",
    "        req_id = cl[1].a.strong.contents[0]\n",
    "\n",
    "print(req_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://nextrequest.cabq.gov/requests'\n",
    "url_det = base_url + '/' + '18-3645'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://nextrequest.cabq.gov/requests/18-3645'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req_det = requests.get('https://bainbridgewa.nextrequest.com/requests/18-21')\n",
    "html = req_det.text.encode('utf8')\n",
    "soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "rdl = soup.find_all(id = 'request-text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"small-12 columns\" id=\"request-text\">\n",
      "      <p>BLD# 7541-1999. 10253 NE Darden Lane, Bainbridge Island, WA.Â  Single family residence house plans BLD #7541-1999.</p>\n",
      "    </div>]\n"
     ]
    }
   ],
   "source": [
    "print(rdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time= soup.find_all('span', class_ = 'time-quotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"time-quotes\">\n",
       "   January 22, 2018,  4:03pm\n",
       " </span>, <span class=\"time-quotes\">\n",
       "   January 17, 2018,  9:40am\n",
       " </span>, <span class=\"time-quotes\">\n",
       "   January 17, 2018,  9:38am\n",
       " </span>, <span class=\"time-quotes\">\n",
       "   January 12, 2018,  5:55pm\n",
       " </span>, <span class=\"time-quotes\">\n",
       "   January  5, 2018, 12:10pm\n",
       " </span>, <span class=\"time-quotes\">\n",
       "   January  5, 2018, 12:10pm\n",
       " </span>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "close_date = time[0].get_text().lstrip().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'January 22, 2018,  4:03pm'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "req_det = requests.get('https://nextrequest.cabq.gov/requests/18-554')\n",
    "html = req_det.text.encode('utf8')\n",
    "soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "rdl = soup.find_all(id = 'request-text')\n",
    "print(rdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"small-12 columns\" id=\"request-text\">\n",
      "      Public records in Mississippi \n",
      "    </div>]\n",
      "[<p class=\"request_date\">\n",
      "          <strong>January 16, 2018</strong>\n",
      "           via web\n",
      "        </p>]\n",
      "NaN\n"
     ]
    }
   ],
   "source": [
    "req_det = requests.get('https://oaklandca.nextrequest.com/requests/RT-24968')\n",
    "html = req_det.text.encode('utf8')\n",
    "soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "rdl = soup.find_all(id = 'request-text')\n",
    "print(rdl)\n",
    "try:\n",
    "    #request = rdl[0].p.contents[0].lstrip().rstrip()\n",
    "    request = rdl[0].get_text().rstrip().lstrip()\n",
    "except:\n",
    "    request = 'error'\n",
    "    print(req_id)\n",
    "rd = soup.find_all('p', class_= \"request_date\") \n",
    "print(rd)\n",
    "try:\n",
    "    create_date = re.findall(r'([^\\n]+)', rd[0].get_text().lstrip().rstrip())[0]\n",
    "except:\n",
    "    create_date = 'error'\n",
    "    print(req_id)\n",
    "time= soup.find_all('span', class_ = 'time-quotes')\n",
    "if status == 'closed' and len(time)>0:\n",
    "    close_date = time[0].get_text().lstrip().rstrip()\n",
    "else:\n",
    "    close_date = 'NaN'\n",
    "    \n",
    "print(close_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time = soup.find_all('div', class_ = 'request-history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_date = re.findall(r'([^\\n]+)', rdl[0].get_text().lstrip().rstrip())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time= soup.find_all('span', class_ = 'time-quotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'June 14, 2018, 11:12am'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time[0].get_text().lstrip().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "June 28, 2018,  2:04pm\n"
     ]
    }
   ],
   "source": [
    "if 'Request Published' in rl[0].get_text() or 'Request Closed' in rl[0].get_text():\n",
    "    close_date = rl[0].find_all('span', class_ = 'time-quotes')[0].get_text().lstrip().rstrip()\n",
    "    print(close_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data from Albuquerque Next Request portal which has different url/data format than others\n",
    "\n",
    "cities = ['Albuquerque']\n",
    "\n",
    "for city in cities:\n",
    "    fn = '{}3.csv'.format(city)\n",
    "    with open(fn, 'w') as csvfile:\n",
    "        fieldnames = ['Reference No', 'Request Status', 'requester', 'request', 'departments', 'cost', 'PoC', 'Create Date', 'Close Date']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames = fieldnames, delimiter = ',')\n",
    "\n",
    "        writer.writeheader()\n",
    "        base_url = 'https://nextrequest.cabq.gov/requests'\n",
    "        alb2_url = 'https://nextrequest.cabq.gov/requests?requests_smart_listing[page]=260'\n",
    "        req = requests.get(alb2_url)\n",
    "\n",
    "        for i in range(260, 261):\n",
    "            html = req.text.encode('utf8')\n",
    "            soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "            rl = soup.tbody.find_all('tr', class_ = \" demo-data-false\") \n",
    "            for prr in rl:\n",
    "                cl = list(prr.children)\n",
    "                # get information from main page\n",
    "                req_id = cl[1].a.strong.contents[0]\n",
    "                status = cl[3]['class'][1]\n",
    "                requester = cl[5].get_text().lstrip().rstrip()\n",
    "                departments = cl[9].get_text().lstrip().rstrip()\n",
    "                cost = cl[11].get_text().lstrip().rstrip().strip('$')\n",
    "                poc = cl[13].get_text().lstrip().rstrip()\n",
    "                \n",
    "                #get information from request detail page\n",
    "                url_det = base_url + \"/\" + req_id\n",
    "                req_det = requests.get(url_det)\n",
    "                html = req_det.text.encode('utf8')\n",
    "                soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "                rdl = soup.find_all(id = 'request-text')\n",
    "                try:\n",
    "                    #request = rdl[0].p.contents[0].lstrip().rstrip()\n",
    "                    request = rdl[0].get_text().rstrip().lstrip()\n",
    "                except:\n",
    "                    request = 'error'\n",
    "                    print(req_id)\n",
    "                rd = soup.find_all('p', class_= \"request_date\") \n",
    "                create_date = re.findall(r'([^\\n]+)', rd[0].get_text().lstrip().rstrip())[0]\n",
    "                time= soup.find_all('span', class_ = 'time-quotes')\n",
    "                if status == 'closed' and len(time) > 0:\n",
    "                    close_date = time[0].get_text().lstrip().rstrip()\n",
    "                else:\n",
    "                    close_date = 'NaN'\n",
    "                \n",
    "                #write to csv\n",
    "                writer.writerow({'Reference No': req_id,\n",
    "                                'Request Status': status,\n",
    "                                'requester': requester,\n",
    "                                'request': request,\n",
    "                                'departments': departments,\n",
    "                                'cost': cost,\n",
    "                                'PoC': poc,\n",
    "                                'Create Date': create_date,\n",
    "                                'Close Date': close_date})\n",
    "                \n",
    "            url = 'https://nextrequest.cabq.gov/requests?requests_smart_listing[page]={}'.format(i)\n",
    "            req = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "westsacramento\n",
      "https://westsacramento.nextrequest.com/requests\n",
      "901\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "# Scrape data from Next Request portals from list of cities given below\n",
    " \n",
    "cities = ['bainbridgewa', 'cityoflascruces', 'mercerisland', 'miami','middleboroughma', 'nola', 'oaklandca', \n",
    "          'providenceri', 'sanfrancisco', 'vallejo','westsacramento']\n",
    "\n",
    "for city in cities:\n",
    "    print(city)\n",
    "    fn = '{}.csv'.format(city)\n",
    "    with open(fn, 'w') as csvfile:\n",
    "        fieldnames = ['Reference No', 'Request Status', 'requester', 'request', 'departments', 'PoC', 'Create Date', 'Close Date']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames = fieldnames, delimiter = ',')\n",
    "\n",
    "        writer.writeheader()\n",
    "        url = 'https://{}.nextrequest.com/requests'.format(city)\n",
    "        print(url)\n",
    "        \n",
    "        req = requests.get(url)\n",
    "        html = req.text.encode('utf8')\n",
    "        soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "        \n",
    "        num_rec = int(soup.find_all('h2')[1].span.get_text().lstrip().rstrip())\n",
    "        print(num_rec)\n",
    "        num_page = math.ceil(num_rec/25)\n",
    "        print(num_page)\n",
    "\n",
    "        for i in range(2, num_page + 2):\n",
    "            rl = soup.tbody.find_all('tr', class_ = \" demo-data-false\") \n",
    "            for prr in rl:\n",
    "                cl = list(prr.children)\n",
    "                # get information from main page\n",
    "                req_id = cl[1].a.strong.contents[0]\n",
    "                status = cl[3]['class'][1]\n",
    "                requester = cl[5].get_text().lstrip().rstrip()\n",
    "                departments = cl[9].get_text().lstrip().rstrip()\n",
    "                poc = cl[11].get_text().lstrip().rstrip()        \n",
    "                \n",
    "            #get information from request detail page\n",
    "                url_det = url + \"/\" + req_id\n",
    "                req_det = requests.get(url_det)\n",
    "                html = req_det.text.encode('utf8')\n",
    "                soup = bs4.BeautifulSoup(html, \"html5lib\")\n",
    "                rdl = soup.find_all(id = 'request-text')\n",
    "                try:\n",
    "                    rdlz = rdl[0]\n",
    "                except:\n",
    "                    print(url_det)\n",
    "                try:\n",
    "                    #request = rdl[0].p.contents[0].lstrip().rstrip()\n",
    "                    request = rdlz.get_text().rstrip().lstrip()\n",
    "                except:\n",
    "                    request = 'error'\n",
    "                    print(url_det)\n",
    "                rd = soup.find_all('p', class_= \"request_date\") \n",
    "                rdz = rd[0] \n",
    "                try:\n",
    "                    create_date = re.findall(r'([^\\n]+)', rdz.get_text().lstrip().rstrip())[0]\n",
    "                except:\n",
    "                    create_date = 'error'\n",
    "                    print(url_det)\n",
    "                time= soup.find_all('span', class_ = 'time-quotes')\n",
    "                if status == 'closed' and len(time) > 0:\n",
    "                    tz = time[0]\n",
    "                    try:\n",
    "                        close_date = tz.get_text().lstrip().rstrip()\n",
    "                    except:\n",
    "                        close_date = 'NaN'\n",
    "                        print(url_det)\n",
    "                \n",
    "                \n",
    "                #write to csv\n",
    "                writer.writerow({'Reference No': req_id,\n",
    "                                'Request Status': status,\n",
    "                                'requester': requester,\n",
    "                                'request': request,\n",
    "                                'departments': departments,\n",
    "                                'PoC': poc,\n",
    "                                'Create Date': create_date,\n",
    "                                'Close Date': close_date})\n",
    "                    \n",
    "            url_2 = url+'?requests_smart_listing[page]={}'.format(i)\n",
    "            req = requests.get(url_2)\n",
    "            html = req.text.encode('utf8')\n",
    "            soup = bs4.BeautifulSoup(html, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
